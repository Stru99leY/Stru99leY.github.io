[{"title":"Jupyter_Command","url":"/2023/05/12/Jupyter-Command/","content":"Jupyter常用的命令命令模式下常用指令\nm键，可以直接输入markdown格式的内容，y键可以回到代码格式\nShift+回车 运行当前代码块\nCtrl+回车 只运行当前代码块\nAlt+回车 运行当前代码块并向下新建一个代码块\n按b向下新建一个代码块\n按a向上新建一个代码块\n按c复制当前代码块\n按x剪切当前代码块\n按v粘贴当前代码块；shift+v,粘贴到上一个代码块\n按z撤回操作\n按dd两次删除代码块\n\n","categories":["笔记"],"tags":["jupyter notebook","python"]},{"title":"Linux命令","url":"/2023/05/26/Linux%E5%91%BD%E4%BB%A4/","content":"Linux常用指令\n获取登录信息 -w &#x2F;who &#x2F;last &#x2F;lastb\n查看自己使用的 -Shell -ps\n查看命令的说明和位置 -whatis &#x2F;which &#x2F;whereis\n清除屏幕上的内容 -clear\n看帮助文档 -man&#x2F;info &#x2F;–help &#x2F;apropos\n查看系统和主机名 -uname &#x2F;hostname\n时间和日期 -date &#x2F;cal\n重启和关机 -reboot &#x2F;shutdown\n\n文件和文件夹操作\n创建&#x2F;删除空目录 -mkdir &#x2F;rmdir\n\n创建&#x2F;删除文件 -touch &#x2F;rm\n\ntouch命令用于创建空白文件或修改文件时间。在linux系统中一个文件有三种时间\n更改内容的时间 -mtime\n更改权限的时间 -ctime\n最后访问时间 -atime\n\n\nrm的几个重要参数    -i 交互式删除，每个删除项目都会进行询问  -r 删除目录并递归的删除目录中的文件和目录    -f 强制删除，忽略不存在的文件，没有任何提示\n\n切换和查看当前工作目录 -cd &#x2F;pwd\n\n快捷键 运行结果\ncd 更改工作目录到home。\ncd - 更改工作目录到先前的工作目录。\ncd ˜user_name 更改工作目录到home目录。例如, cd ˜bob 会更改工作目录到用“b录。\n\n\n\n\n查看目录内容 -ls -l 以长格式查看文件和目录-a 显示以点开头的文件（隐藏文件）-R 遇到目录要进行递归展开(继续列出目录下面的文件和目录)-d 只列出目录，不列出其他内容-S &#x2F;-t 按大小&#x2F;时间排序\n\n查看文件内容 -cat &#x2F; tac &#x2F; head &#x2F; tail &#x2F; more &#x2F; less &#x2F; rev &#x2F; od\n\n拷贝&#x2F;移动文件 - cp &#x2F; mv\n ​\tmv命令：将文件剪切到当前文件夹并重新命名，执行后，原来的文件消失，新的文件创建成功，新的文件实际上就是原来的文件，只不过名字变了\n\n文件重命名 -rename\n\n查找文件和查找内容 -find &#x2F; grep\n\ngrep在搜索字符串是可以使用正则表达式，如果需要使用正则表达式可以用grep -E 或者直接使用egrep\n\n\n创建链接和查看链接 ln 具体语法:\n\n\nln [选项] [源文件] [链接]\n\n\n系统命令\npsps -ef表示显示所有进程，ps -ef|grep其中grep表示筛选包含特定关键词\n\ndf\ndf 以磁盘分区为单位查看文件系统，可以获取硬盘被占用了多少空间，目前还剩下多少空间等信息。\n例如，我们使用df -h命令来查看磁盘信息， -h 选项为根据大小适当显示\n相关命令:\n\ndf -hl：查看磁盘剩余空间\ndf -h：查看每个根路径的分区大小\n**du -sh [目录名]**：返回该目录的大小\n**du -sm [文件夹]**：返回该文件夹总M数\n**du -h [目录名]**：查看指定文件夹下的所有文件大小（包含子文件夹）\n\n\nlsof\nlsof（list open files）是一个列出当前系统打开文件的工具。\n命令格式：\nlsof [参数][文件]\n具体使用\n\n\nshell命令\n复制文件和目录 scp 命令scp是linux系统下基于ssh登录进行安全的远程文件拷贝指令；scp是加密的，rcp是不加密的，sco是rcp的加强版，简易语法如下：\nscp [可选参数] file_source file_target\n\n\n\n\n","categories":["笔记"],"tags":["Linux"]},{"title":"MarkDown语法","url":"/2023/06/04/MarkDown%E8%AF%AD%E6%B3%95/","content":"这是一级标题这是二级标题这是三级标题换行的话在末尾打两个空格然后回车就能换行\n斜文字 ctrl+i文字加粗 ctrl+b  \n插入图片直接复制进来就可以  \n表格\n\n\n大明\n小明\n小红\n\n\n\n101\n21\n23\n\n\n冒号在哪边就往哪边对齐\n\n\n\n\n链接这是一个链接\ncodeprintf(&quot;hello world&quot;);\n\n\n实心点\n\n换行:&lt;br/&gt;\n","categories":["MarkDown"],"tags":["语法"]},{"title":"Python随笔","url":"/2023/06/03/Python%E9%9A%8F%E7%AC%94/","content":"Python的一些不熟悉的知识一些小知识，想到什么写什么\n列表与数组\n\npython中的列表可以包含不同类型的元素，如字符串、整数、浮点数等，而数组通常包含相同类型的元素\npython中列表是动态数组，可以自动调整大小，而数组再创建时需要指定大小，一旦创建后，大小就不能再改变，因此在插入或删除元素时需要手动。\n数组相当于列表来说在访问元素时性能更好，以为列表是动态数组插入或删除元素可能会有些性能损失。\n\n\n元组元组是一个集合是有序的和不可改变的元组使用小括号(),列表使用方括号[]元组的创建很简单，只需要在括号中添加元素，使用逗号隔开即可\n\n集(Set)集合是无序和无索引的集合，可以用来快速判断一个元素是否在某个集合中，在py中用{}表示\n\n添加一个项目到集合用add()\n thiset=&#123;&quot;apple&quot;,&quot;banana&quot;,&quot;cherry&quot;&#125;thiset.add(&quot;orange&quot;)\n\n添加多个项目用update()\n   thiset=&#123;&quot;apple&quot;,&quot;banana&quot;,&quot;cherry&quot;&#125;thiset.update([&quot;orange&quot;, &quot;mango&quot;, &quot;grapes&quot;])\n\n删除用Remove()或discard()方法 如果删除的项目不存在，remove()会引发错误，而discard()不会引发错误\n\npop()删除最后一项\n\nclear()清空集合 …\n\n\n\n字典\n\n使用values()函数返回字典的值\n for x in thisdict.values():  print(x)\n\n使用items()循环遍历键和值\n for x,y in thisdict.items():  print(x,y)\n\ndict()创建字典\n epdict = dict()print(epdict) # 输出&#123;&#125;print(len(epdict)) # 输出0print(type(epdict)) # &lt;class &#x27;dict&#x27;&gt;\n\n\n字典的访问\n使用中括号[ ]  \n dic = &#123;&#x27;key1&#x27;: &#x27;value1&#x27;, &#x27;key2&#x27;: &#x27;value2&#x27;, &#x27;key3&#x27;: &#x27;value3&#x27;&#125;print(dic[&#x27;key1&#x27;])  # 输出 value1\n\n使用get( )方法 dic.get(key,default&#x3D;None)它用于从字典中获取指定键key对应的值，键不存在返回一个默认值deafult\n dic = &#123;&#x27;key1&#x27;: &#x27;value1&#x27;, &#x27;key2&#x27;: &#x27;value2&#x27;, &#x27;key3&#x27;: &#x27;value3&#x27;&#125;print(dic.get(&#x27;key1&#x27;)) # 输出value1print(dic.get(&#x27;key4&#x27;)) # 输出 None\n\n除以上外，可以通过keys(),values()和items()方法获取字典中所有的键、值和键值对。例如:\n dic = &#123;&#x27;key1&#x27;: &#x27;value1&#x27;, &#x27;key2&#x27;: &#x27;value2&#x27;, &#x27;key3&#x27;: &#x27;value3&#x27;&#125;print(dic.keys()) # 输出 dict_keys([&#x27;key1&#x27;, &#x27;key2&#x27;, &#x27;key3&#x27;])print(dic.values()) # 输出 dict_values([&#x27;value1&#x27;, &#x27;value2&#x27;, &#x27;value3&#x27;])print(dic.items()) # dict_items([(&#x27;key1&#x27;, &#x27;value1&#x27;), (&#x27;key2&#x27;, &#x27;value2&#x27;), (&#x27;key3&#x27;, &#x27;value3&#x27;)])\n\n 注意:keys()、values()、items()方法返回的是一种特殊的视图对象，而不是普通的列表。要转换成列表可以用list()函数\n\n\n\n\n\n频率看到计算频率等问题直接联想到以下代码:\nfor i in nums:  dic[i] = dic.get(i,0) + 1\n\n解包解包是指将一个序列或元组中的元素分别赋值给变量。可迭代对象是指可以被迭代器遍历的对象，例如列表、元组、字符串等。而解包则是指将一个序列或元组中的元素分别赋值给多个变量的操作。\n\n*args和**kwargs\n\n*args（arguments）允许你传递零个或多个位置参数给一个函数。它会将传递给函数的位置参数收集到一个元组(tuple)中。在函数定义中，*args以星号(*)的形式出现，但你可以用任何变量名代替它。\n\npythonCopy codedef example_function(*args):    for arg in args:        print(arg)example_function(1, 2, 3, 4, 5)# Output:# 1# 2# 3# 4# 5\n\n\n**kwargs（keyword arguments）允许你传递零个或多个带有关键字的参数给一个函数。它会将传递给函数的关键字参数收集到一个字典(dict)中，其中关键字作为字典的键，相应的值作为字典的值。在函数定义中，**kwargs以双星号(**)的形式出现，但你同样可以用任何合法的变量名代替它。\n\npythonCopy codedef example_function(**kwargs):    for key, value in kwargs.items():        print(f&quot;&#123;key&#125;: &#123;value&#125;&quot;)example_function(a=1, b=2, c=3)# Output:# a: 1# b: 2# c: 3\n\n区别总结：\n\n*args用于接收位置参数，收集为一个元组(tuple)。\n**kwargs用于接收关键字参数，收集为一个字典(dict)。\n你可以在同一个函数中同时使用*args和**kwargs，但是*args必须在**kwargs之前。\n参数名args和kwargs只是一种约定，你可以用其他任何合法的变量名替代它们，但通常使用它们是为了提高代码的可读性和清晰度。\n\n\n取整使用int取整大于0的数向下取整，小于0的数向上取整\n\n匿名函数  lambda函数是个匿名函数，可以使用任意数量的参数，但是只有一个表达式。\n\n用法\n\nlambda arguments:expression# ep1.返回一个加10的数x=lambda a:a+10# ep2.可以使用任意数量的参数：x=lambda a,b,c:a+b+cprint(x(1,2,3))\n\n迭代器列表、元组、字典都是可迭代的对象，对于这些对象都有一个获取迭代器iter()方法\nmytuple = (&quot;apple&quot;, &quot;banana&quot;, &quot;cherry&quot;)myit = iter(mytuple)print(next(myit))print(next(myit))print(next(myit))\n\n创建一个返回数字的迭代器\nclass MyNumbers:  def __iter__(self):    self.a=1    return self  def __next__(self):    x=self.a    self.a +=1    return xmyclass = MyNumbers()myiter = iter(myclass)print(next(myiter))print(next(myiter))print(next(myiter))print(next(myiter))print(next(myiter))\n\nStoplteration关键字如果有足够的next()语句，为了防止迭代继续进行，可以使用StopIteration语句ep1迭代20次后停止\nclass MyNumbers:  def __iter__(self):    self.a=1    return self    def __next__(self):    if self.a &lt;= 20      x = self.a      self.a += 1      return x    else:      raise StopIterationmyclass = MyNumbers()myiter = iter(myclass)for x in myiter:  print(x)\n\nraise\nraise 关键字用于引发异常\nep:如果x&lt;0则引发错误并停止程序\nx = -1if x &lt; 0:    raise Excpetion(&quot;error&quot;)\n\n\n\nself在python中,self是一个惯用的参数名称。在定义一个类的方法时，需要将self作为第一个参数传递给方法，当我们定义一个类的方法时，那么在调用该方法时，实际上是通过类的实例对象来调用这个方法，因此不需要显式传递self参数；如果我们在类的方法中访问类的属性或者方法，那么必须使用self来访问(self.func)。如果我们在方法中直接访问属性或者方法，py会解释为局部变量或函数，从而导致错误。\n\n运算符\n\n&#x2F;&#x2F; –&gt; 取整数-往小的方向取整数ep\n\n\n9&#x2F;&#x2F;24\n\n\n\n\n** –&gt; 幂 返回x的y次幂\n逻辑运算符\n\n\n与 and –&gt; x and y\n或 or  –&gt; x  or y\n非 not –&gt; x not y\n\n\n二维数组初始化\n  # 创建一个rows行cols列的二维数组，初始值为0matrix = [[0 for _ in range(cols)] for _ in range(rows)]# 创建一个rows行cols列的二维数组，初始值为matrix = [[0] * cols for _ in range(row)]\n\n字符串格式化例子:\nname = &quot;张三&quot;age = 18print(&quot;我叫%s，今年%d岁。&quot; % (name, age)) # 输出: &quot;我叫张三，今年18岁。&quot;\n\ntrytry和except语句来处理异常，try语句块用于包裹可能会引发异常的代码，而except来处理可能引发的异常，除此之外还可以使用else和finally语句块来进一步处理异常。else语句块中的代码会在try语句块中没有发生异常时执行，而finally语句中的代码无论是否发生异常都会执行。\ntry:  x = 10 / 2except ZeroDivisionError:  print(&quot;除数不能为零&quot;)else:  print(&quot;结果为:&quot;, x)finally:  print(&quot;无论是否发生异常，都会执行这里的代码&quot;)\n\nexcept异常由以下几种:\n  Exception：所有异常的基类，可以捕获任何类型的异常。ZeroDivisionError：除以零时引发的异常。TypeError：类型不匹配时引发的异常。ValueError：值不合法或无效时引发的异常。IndexError：索引超出范围时引发的异常。KeyError：字典中使用不存在的键时引发的异常。FileNotFoundError：打开或读取文件时找不到文件时引发的异常。NameError：尝试访问不存在的变量或函数时引发的异常。ImportError：导入模块失败时引发的异常。IOError：输入/输出操作失败时引发的异常。\n\n推导式用于快速创建列表、字典、集合等可迭代对象，一般有以下几种:\n\n列表推导式 语法形式计为:[expression for item in iterable if condition]或者[expression for item in iterable] 示例:\n\n过滤长度小于或者等于3的字符串列表，并将剩下的转换成大写字母:\n\n names = [&#x27;Bob&#x27;,&#x27;Tom&#x27;,&#x27;alice&#x27;,&#x27;Jerry&#x27;,&#x27;Wendy&#x27;,&#x27;Smith&#x27;]new_names = [name.upper()for name in names if len(name)&gt;=3]print(new_names)# 输出[&#x27;ALICE&#x27;, &#x27;JERRY&#x27;, &#x27;WENDY&#x27;, &#x27;SMITH&#x27;]\n\n字典推导式  语法形式为:{key_expr:value for value in collction} 或者 {key_expr:value for value in collction if condition}  示例:\n\n使用字符串及其长度创建字典\n\n  listdemo = [&#x27;Google&#x27;,&#x27;Runoob&#x27;, &#x27;Taobao&#x27;]newdic = &#123;key:len(key) for key in listdemo&#125;newdic# 输出&#123;&#x27;Google&#x27;: 6, &#x27;Runoob&#x27;: 6, &#x27;Taobao&#x27;: 6&#125;\n\n集合推导式 语法形式为:{expression for item in Sequence} 或者 {expression for item in Sequence if condition} 示例:\n\n判断不是abc的字母并输出\n\n a = &#123;x for x in &#x27;abracadabra&#x27; if x not in &#x27;abc&#x27;&#125;a# 输出 &#123;&#x27;d&#x27;,&#x27;r&#x27;&#125;type(a)# 输出 &lt;class &#x27;set&#x27;&gt;\n\n元组推导式(生成器表达式) 可以利用range区间、元组、列表、字典和集合等数据结构类型，快速生成一个满足指定需求的元组，语法形式为:(expression for item in Sequence) 或者 (expression for item in Sequence if condition)\n\n\n\n元组推导式和列表推导式用法完全相同，只是元组推导式是用()圆括号,列表推导式是用[]中括号，类外元组推导式返回的结果也是一个生成器对象 示例:\n\n\n\n\n生成一个包含数字1-9的元组：\n\n a= (x for x in range(1,10))a# &lt;generator object &lt;genexpr&gt; at 0x7faf6ee20a50&gt; 返回是是生成器对象tuple(a)# 转换为元组 (1, 2, 3, 4, 5, 6, 7, 8, 9)\n\n\nstack下标问题正数索引(从0开始)，列表元素的顺序从左到右。而对于负数索引(从-1开始),元素顺序是从右到左；相当于一个是入栈一个是出栈\n\n\npy函数\nremove()用于从列表中删除指定元素；语法格式为: list_name.remove(element) #element 任意数据类型\n\n删除普通类型元素 删除列表中存在的数字或者字符串;不存在会报错，引发valueError异常\n list1 = [&#x27;zhangsan&#x27;, &#x27;lisi&#x27;, 1, 2]list1.remove(1)list1.remove(&#x27;lisi&#x27;)\n\n删除对象类型元素  \n list1=[1,2,[3,4],(5,6)]a=[3,4]b=(5,6)list1.remove(a) # 删除列表类型list1.remove(b) # 删除元组类型\n\n删除多个元素，可以使用循环来实现\n # 创建一个列表fruits = [&quot;apple&quot;, &quot;banana&quot;, &quot;orange&quot;, &quot;peach&quot;, &quot;banana&quot;, &quot;kiwi&quot;]      # 删除所有的 &quot;banana&quot; 元素while &quot;banana&quot; in fruits:    fruits.remove(&quot;banana&quot;)      print(fruits)  # [&quot;apple&quot;, &quot;orange&quot;, &quot;peach&quot;, &quot;kiwi&quot;]\n\n 注意:在使用 remove() 函数时要谨慎，特别是在迭代列表的过程中,当删除一个元素后会导致后面的元素索引发生变化从而可能导致发生错误\n\nenumerate()enumerate()是python中的一个内置函数，将一个可迭代对象转换为一个枚举对象，同时返回每个元素的索引和值，常用于for循环中获取中获取每个元素的位置信息。用法: enumerate(iterable,start&#x3D;0)其中，iterable表示要枚举的可迭代对象，start表示起始索引\n\nord()ord()是一个内置函数，用于获取给定字符的Unicode码点值，ord()函数接受单个字符作为参数；\nord(&#x27;a&#x27;) # 返回97\n\ndefaultdict()defaultdict是python中的一个内置字典子类，重载了一个方法来实现默认值的设定。在创建defaultdict对象，需要提供一个参数作为默认值或者一个函数用来生成默认值。比如有一个字典对其中的值进行累加操作，如果某个键不存在，则将其值设置为0，使用defaultdict可以避免手动判断键是否存在的过程:\nfrom collections import defaultdictd = defaultdict(int)# 对字典中的值进行累加操作for i in &#x27;mississippi&#x27;：  d[i]+=1print(d) # 输出：defaultdict(&lt;class &#x27;int&#x27;&gt;, &#123;&#x27;m&#x27;: 1, &#x27;i&#x27;: 4, &#x27;s&#x27;: 4, &#x27;p&#x27;: 2&#125;)\n\nsorted()返回一个新的排好序的列表，而不改变原始列表例如有一个数字列表\nnum = [4, 2, 7, 1, 3]\n\n我们可以使用sorted()进行排序、会返回一个新的列表，但是排序后会改变数字原有的索引值\n  sorted_list = sorted(nums)\n\n若不想改变原数组的索引值可以用以下方法\n  # 对nums的元素进行排序，排序的依据是nums中对应的位置，对于nums中的每个元素k，使用nums[k]的值作为排序关键字sorted(nums,key=lambda k:nums[k]])# 使用enumerate()生产一个包含元素索引和值的可迭代对象，按照值进行排序，不改变原有元素的索引值sorted_nums = soreted(enumerate(nums),key=lambda x:x[1])\n\n对于上述第二种方法，enumerate(nums)会返回一个(index,value)的元组.  \n\nlist()list()是一个空列表的构造函数。当调用list()时，它会返回一个空列表[]。这个空列表可以用来存储任意类型的元素，并且可以根据需要进行修改、添加或删除元素。\n\n基本数据结构操作\npython建立二叉树(递归)\n\n","categories":["Python"],"tags":["基础知识"]},{"title":"SQL补充","url":"/2023/10/31/SQL%E8%A1%A5%E5%85%85/","content":"SQL的特征\n大小写不敏感\n\n需要以;结尾\n\n注释:\n\n单行注释: – 注释内容 (– 后面一定要有一个空格) # 注释内容，可以不加空格\n多行注释:&#x2F;* 注释内容 *&#x2F;\n\n\n\nSQL语言的分类\nDDL 数据定义\nDML 数据操作\nDCL 数据控制\nDQL 数据查询\n\nDDL库管理\n查看当前使用的数据库\n\n​\t\tselect database();\n\n查看有哪些表(的先选择数据库)\n\n​\t\tshow tables \n\n删除表\ndrop table 表名称;\ndrop table if exists 表名称;\n\n创建表\ncreate table 表名称(\t列名称 列类型,    列名称 列类型，    .....)-- 列类型有int         -- 整数float       -- 浮点数varchar(长度)-- 文本，长度为数字，做最大长度限制date        -- 日期类型timestamp   -- 时间戳类型\n\nDML数据插入insert基础语法:\ninsert into 表[(列1,列2,....,列N)] values(值1,值2,....,值N)[(值1,值2,....,值N),(值1,值2,....,值N),....,(值1,值2,....,值N)]# []代表可选\n\n数据删除 delete基础语法:\ndelete from 表名称 [where 条件判断];# 条件判断:列 操作符 值\n\n数据更新 update基础语法:\nupdate 表名 set 列=值 [where 条件判断];\n\nDQL 数据查询查询基础SQL中，通过select关键字开头的SQL语句，来进行数据的查询,查询也可以指定条件\neg：\nCREATE table student(\tid int,\tname VARCHAR(20),\tage int\t);INSERT INTO student VALUES(10001,&#x27;周杰伦&#x27;,31),(10002,&#x27;王力鸿&#x27;,33),(10003,&#x27;林俊杰&#x27;,35),(10004,&#x27;张学友&#x27;,36),(10005,&#x27;刘德华&#x27;,30);# 查询id 和 name 两个列select id,name from student;# 查询全部列select id,name,age from student;select * from student;# 查询id，name两个列，年龄小于33select id,name from student where age&lt;33;\n\n\n模糊匹配\n\nSQL模式\n在使用SQL模式时，不能使用&#x3D;或!&#x3D; 而使用like或not like比较运算符。\n语法: SELECT 字段 FROM 表 WHERE 某字段 Like 条件\nSQL 提供两种匹配模式:\n\n百分号(%):表示任意个或多个字符，可以匹配任意类型和长度的字符\n示例:\nselect * from character where name like &#x27;%刘%&#x27;;# 匹配姓名为&quot;刘&quot;类型的数据select * from character where name like &#x27;%马%梅%&#x27;;# 只能匹配姓名为&quot;...马...梅...&quot;的数据\n\n下划线(_)：表示任意单个字符。匹配单个任意字符，它常用来限制表达式的字符长度语句(可以代表一个中文字符)\n示例:\nselect * from character where name like &#x27;_三_&#x27;;# 匹配&quot;x三x&quot;类型的数据，前后只有一个字符\n\n\n正则模式\n\nmysql正则表达式仅仅是sql语言的一个子集，可以匹配基本的字符、字符串。\nselect * from name where name regexp &#x27;jack&#x27;;# 可以检索处name中所有包含jack的行\n\n. 匹配除了\\n之外的任意单个字符\n\n^ 匹配字符串开始位置，如查询所有行李的人\nselect name from table where name regexp &#x27;^李&#x27;;\n\n$ 匹配字符串结束位置，如查询人名结尾的情况\n\n进行OR匹配\n为搜索两个串之一（或者这个串，或者为另外一个串），使用|。|作为OR操作符，表示匹配其中之一。可以给出两个以上的OR条件。\nselect * from products where pro_id REGEXP &#x27;1000|2000&#x27;;# 这样就1000和2000都能匹配并返回，当然，使用多个|就可以匹配多个串\n\n[  ] 匹配任何单一字符，是另一种形式的OR语句，可缩写的OR语句\n例如，匹配范围：[0123456789]可以匹配0到9，[1-4][4-9]也是合法的范围。此外，范围不一定只是数值的，[a-z]匹配任意字母字符。\n\n[^….] 匹配不包含在[ ]的字符\n\n匹配特殊字符使用\\进行转义\n\n&gt;（1）\\\\-  表示查找-&gt;（2）\\\\.  表示查找.&gt;（3）\\\\f  表示换页&gt;（4）\\\\n 表示换行&gt;（5）\\\\r  表示回车&gt;（6）\\\\t  表示制表&gt;（7）\\\\v  表示纵向制表&gt;注意:匹配\\本身，需要使用\\\\\\\n\n\n匹配字符类\n\n&gt;（1）[:alnum:]  任意字母和数字（同[a-zA-Z0-9]）&gt;（2）[:alpha:]   任意字符（同[a-zA-A]）&gt;（3）[:blank:]   空格和制表符（同[\\\\t]）&gt;（4）[:digit:]    任意数字（同[0-9]）&gt;（5）[:lower:]   任意小写字母（同[a-z]）&gt;（6）[:upper:]  任意大写字母（同[A-Z]）&gt;（7）[:space:]  包括空格在内的任意空白字符（同 [\\\\f\\\\n\\\\t\\\\r\\\\v]）&gt;（8）[:cntrl:]    ASCII控制字符（ASCII 0到31和127）&gt;（9）[:graph:]  与[&quot;print:]相同，但不包括空格&gt;（10）[:print:]   任意可打印字符&gt;（11）[:punct:] 既不在 [:alnum:] 又不在 [:cntrl:] 中的任意字符&gt;（12）[:xdigit:] 任意十六进制数字（同 [a-fA-F0-9]）\n\n\n匹配多个示例，关于重复元字符\n\n\n\n元字符\n说明\n\n\n\n*\n0个或多个匹配\n\n\n+\n1个或多个匹配（等于 {1, }）\n\n\n？\n0个或1个匹配（等于 {0, 1}）\n\n\n{n}\n指定数目的匹配\n\n\n{n, }\n不少于指定数目的匹配\n\n\n{n ,m}\n匹配数目的范围（m不超过255）\n\n\n\n定位符\n\n&gt;^     文本的开始&gt;$    文本的末尾&gt;[[:&lt;:]] 词的开始&gt;[[:&gt;:]] 词的结尾\n\n\n\nlike contact 模糊查询\ncontact(str1,str2,…)函数返回结果为连接参数产生的字符串\nselect * from role where name like contact(&quot;%&quot;,&quot;三&quot;,&quot;%&quot;);# 即匹配姓名为“唐三”，“唐三藏”等类型的数据数据；\n\n\n\n子查询子查询指一个查询语句嵌套在另一个查询语句内部的查询，从MYSQL4.1开始引入。\n\n子查询在主查询之前一次执行完成\n\n子查询的结果被主查询使用\n\n注意:\n\n子查询要包含在括号内\n\n讲子查询放在比较条件的右侧\n\n单行操作符对应单行子查询，多行操作符对应多行子查询\n(按内查询是否被执行多次，从内查询返回的结果条目数来区分单行和多行子查询)\n\n\n\n\n分组聚合分组聚合应用场景非常多，如:统计班级中，男生和女生的人数。这种需求就需要:\n\n按性别分组\n统计每个组的人数\n\n这就称之为分组聚合\n一个sql中可以写多个聚合\n基础语法:\nselect 字段|聚合函数 from 表 [where 条件] group by 列# 聚合函数有:- sum(列) 求和- avg(列) 平均- min(列) 求最小值- max(列) 求最大值- count(列|*) 求数量\n\n注意:非聚合函数中，group by 中有谁，select中才能写谁\n排序分页结果排序如果没有使用排序操作，默认情况下查询返回的数据是按照添加数据的顺序显示的。\n使用order by关键字，指定某个列进行排序，(列的别名只能再order by中使用，不能在where中使用)语法:\nselect 列|聚合函数|* from 表 where ...group by...order by... [ASC|desc]# ASC 升序，desc 降序 不选择默认升序排列\n\n使用limit的好处:\n约束返回结果的数量可以减少数据表的网络传输量 ，也可以提升查询效率 。\n\n在对多列进行排序的时候，首先排序的第一列必须有相同的列值，才会对第二列进行排序。如果第一列数据是唯一的，将不再对第二列进行排序。\n\n结果分页排序使用limit关键字，对查询结果进行数量限制或分页显示\n注意:limit子句必须放在整个select语句的最后;在 MySQL 中，LIMIT子句不支持使用算术表达式\nselect * from student limit 10,5;# 表示从第十条开始向后取五条\n\nJOIN关联查询除了使用1张表外，可能会同时使用多张表，常见的多表查询方式有:\n\nfrom 多表\n基础语法:\nselect ... from 表1 as 别名1，表2 as 别名2,...,表N[where 链接条件]\n\n直接再from中写多个表，通过as可以给出表别名\n直接查询两个表时我们会发现结果会产生笛卡尔积:\n\n\n要改变查询结果，只需要再表名后面加上where关键字，进行条件判断即可;同时查询结果中也有一些列是我们不需要的，只需要再select中选择我们需要查询的列即可:\n\n\n\n\ninner join \n内关联，由上面的from多表我们可以看出，查询结果会出现笛卡尔积，尽管可以通过where进行条件过滤但性能不太好；使用内关联能有效改善性能，语法：\nselect ... from 表1 as 别名1 [inner] join 表2 as 别名2 on 连接条件;\n\n因此上述查询语句可改为:\nSELECT s.*,c.name FROM stu as s INNER JOIN class as c on s.class_id=c.id;\n\n内关联本质上就是求交集的过程，若两个表中存在没有交集的数据，那么查询结果中就不会显示\n\nouter join\n如果想解决没有交集的数据，可以使用外关联来显示内关联失败的数据，语法:\nselect ... from 表1 as 别名1 (left|right) [outer] join 表2 as 别名2 on 连接条件;# outer 关键字可以省略# 外关联分左右，必须选择left join 左外关联或者 right join 右外关联，二选一# 左 右 关联的区别是，以谁为核心\n\n拿上面的查询代码来说 left join 就是以stu表为核心，不管class表里面有没有与之匹配的数据，在结果中都会显示stu表中的数据，而对应的class表中的列将会显示null，right join 则反之。\n\n\n函数  流程处理函数可以根据不同的条件，执行不同的处理流程，可以在sql语句中实现不同的条件选择。MySQL中的流程处理函数主要包括if()、IFNULL()、CASE()函数\n\n\n\n函数\n用法\n\n\n\nF(value,value1,value2)\n如果value的值为TRUE，返回value1， 否则返回value2\n\n\nIFNULL(value1, value2)\n如果value1不为NULL，返回value1，否 则返回value2\n\n\nCASE WHEN 条件1 THEN 结果1 WHEN 条件2 THEN 结果2 …. [ELSE resultn] END\n相当于Java的if…else if…else…\n\n\nCASE expr WHEN 常量值1 THEN 值1 WHEN 常量值1 THEN 值1 …. [ELSE 值n] END\n相当于Java的switch…case…\n\n\n","categories":["笔记"],"tags":["mysql"]},{"title":"git随笔","url":"/2023/04/06/git-merge%E7%AC%94%E8%AE%B0/","content":"gitub问题:master和main分支合并问题:github创建仓库后默认分支是main，而本地创建的是master\n\n先给本地master改名\ngit branch -M main//-M对分支重命名\n\n查看所有分支\n git branch -a* main   remotes/origin/main   remotes/origin/master  \n\n删除远程分支master\n  $ git push origin --delete master To https://github.com/     - [deleted]         master  \n\n确认删除情况\n$git branch -a* mainremotes/origin/main\n\n切换当前分支main，也就要保留下来的分支\n $ git checkout main Already on &#x27;main&#x27; \n\n合并分支\n $ git merge remotes/origin/main fatal: refusing to merge unrelated histories \n\n 说明:拒绝合并，需要忽略这个限制，添加”–allow–unrelated-histories”\n\n提交修改\ngit push origin main\n\ngitignore问题当我们创建好文件并且将其推送到github后再创建gitignore文件的话，会发现在gitignore里面添加的文件都不会生效。\n**原因:**git忽略目录中，新建的文件在git中会有本地缓存，如果某些文件已经被纳入了版本管理中，就算是在.gitignore中已经声明了忽略路径也是不起作用的，.gitignore文件只是忽略没有被staged(cached)文件，对于已经被staged文件，加入ignore文件时一定要先从staged移除，才可以忽略\n此时需要执行如下代码:\n# 在idea控制台以此输入如下命令git rm -r --cached .git add .git commit -m &quot;随便写&quot;git push -u origin main\n\n比较不同分支的差异在vscode中比较两个不同分支同一个文件的差异时，在如下设置可查看:\n\n\n点击之后，再选择要查看的文件\n\n","categories":["随笔"],"tags":["git"]},{"title":"Hello World","url":"/2023/05/26/hello-world/","content":"hexo添加新文章hexo new post filename编辑你的文件hexo g -d //完成推送到GitHub就行","categories":["hello"],"tags":["Hello World"]},{"title":"初识BigData-hadoop-hdfs-hvie","url":"/2023/10/23/%E5%88%9D%E8%AF%86BigData-hadoop-hdfs-hvie/","content":"node1:9870\n启动环境(要在 hadoop 用户下）# 1.启动hdfsstart-dfs.sh# 停止stop-dfs.sh# 2.启动yarnstart-yarn.sh# 3.启动历史服务器mapred --daemon start historyserver# 4.启动metastore(在hive目录下)#前台启动 bin/hive --service metastore#后台启动 nohup bin/hive --service metastore &gt;&gt; logs/metastore.log 2&gt;&amp;1 &amp;# 5.启动hive#直接写SQL bin/hive#可供其他客户端链接 bin/hive --service hiveserver2#hiveserver2后台启动 nohup bin/hive --service hiveserver2 &gt;&gt; logs/hiveserver2.log 2&gt;&amp;1 &amp;\n\n单独控制进程的启停。\n\n$HADOOP_HOME&#x2F;sbin&#x2F;hadoop-daemon.sh，此脚本可以单独控制所在机器的进程的启停\n 用法：hadoop-daemon.sh (start|status|stop)(namenode|secondarynamenode|datanode)\n\n$HADOOP_HOME&#x2F;bin&#x2F;hdfs，此程序也可以用以单独控制所在机器的进程的启停\n\n\n用法：hdfs –daemon (start|status|stop) (namenode|secondarynamenode|datanode)\n文件系统的操作命令Hadoop 提供了两套命令:\nhadoop(老版本),用法:hadoop fs\nhdfs（新版本),用法:hdfs dfs\n\n创建文件夹\nhadoop fs -mkdir [-p]  …\nhdfs dfs -mkdir [-p]  …\npath​​为待创建目录\n-p​​选项与 linux mkdir​​一致会沿着路径创建父目录\n\n查看指定目录下内容\n\nhadoop fs -ls [-h] [-R] […]\n\nhdfs dfs -ls [-h] [-R] […]\npath​​指定目录路径\n-h​​人性化显示文件 size\n-R​​递归查看指定目录及其子目录\n\n\n\n上次文件到 HDFS 指定目录下\n\nhadoop dfs -put [-f] [-p]  … \n\nhdfs dfs -put [-f] [-p]  … \n-f​​覆盖目标文件(已存在下)\n-p​​保留访问和修改时间，(客户端所在机器）\ndst​​目标文件系统\n\n\n\n查看 HDFS 文件内容\n\nhadoop fs -cat  …\n\nhdfs dfs -cat  …\nhadoop fs -cat /itcast/words.txthdfs dfs -cat /itcast/profile\n\n读取大文件可以使用管道符配合 more\n\nhadoop fs -cat  | more\n\nhdfs dfs -cat  | more\n\n\n\n下载 HDFS 文件\n\nhadoop fs -get [-f] [-p]  … \n\nhdfs dfs -get [-f] [-p]  … \n下载文件到本地文件系统指定目录，localdst 必须是目录\n-f ​覆盖目标文件\n-p ​保留访问和修改时间，所有权和权限\n\n\n\n拷贝 HDFS 文件\n\nhadoop fs -cp [-f]  … \nhdfs dfs -cp [-f]  … \n\n\n追加数据到 HDFS 文件中\n\nhadoop fs -appendTofile  … \n\nhdfs dfs -appendTofile  … \n将所有给定本地文件的内容追加到给定 dst 文件，若 dst 文件不存在，将创建该文件。如果 &lt;localSrc&gt; ​为 -​，则输入为从标准输入中读取。\n\n\n\nHDFS 数据移动操作\n\nhadoop fs -mv  … \n\nhdfs dfs -mv  … \n可以重命名\n\n\n\nHDFS 数据删除操作\n\nhadoop fs -rm -r [-skipTrash] URI [URI …]\n\nhdfs dfs -rm -r [-skipTrash] URI [URI …]\n-skipTrash ​跳过回收站直接删除\n‍\n#&lt;property&gt;&lt;name&gt;fs.trash.interval&lt;/name&gt;&lt;value&gt;1440&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;fs.trash.checkpoint.interval&lt;/name&gt;&lt;value&gt;120&lt;/value&gt;&lt;/property&gt;# 无需重启集群，在哪个机器配置的，在哪个机器执行命令就生效。回收站默认位置在：/user/用户名(hadoop)/.Trash\n\n\n\nHDFS 储存\nHDFS 副本块数量的配置\n可以在上传文件的时候，临时决定被上传文件以多少个副本存储\nhadoop fs -D dfs.replication=2 -put test.txt /tmp/ ​如该命令，就可以在上传 test.txt ​文件时，临时设置其副本数为 2.\n对于已经存在 HDFS 的文件，修改 dfs.replication ​属性不会生效，可通过命令 hadoop fs -setrep [-R] 2 path ​将指定 path 的内容将会被修改为 2 个副本储存。-R ​选项表示对子目录也生效。\n\nfsck 命令检查文件的副本数\nhdfs dfs path [-file[-blocks[-locations]]]​\n-files ​可以列出路径内的文件状态\n-files -blocks ​输出文件块报告(几个块，多少副本)\n-files -blocks -locations ​输出每一个 block 的详情\n\n\nyarnnode1:8088\n\n启动 historyserver\nmapred --daemon start historyserver​\n\n\nHIVEApache Hive 其 2 大主要组件就算:SQL 解析器以及元数据存储\n​\n\nHive 架构图​\n\n\n元数据存储\n存储在关系数据库如 mysql&#x2F;derby 中。Hive 中的元数据包括表的名字，表的列和分区及其属性，表的属性（是否为外部表等），表的数据所在目录等。Hive 提供了 Metastore 服务进程提供元数据管理功能\n\nDriver 驱动程序，包括语法解析器、计划编译器、优化器、执行器\n完成 HQL 查询语句从词法分析、语法分析、编译、优化以及查询计划的生成。生成的查询计划存储在 HDFS 中，并在随后有执行引擎调用执行。\n这部分内容不是具体的服务进程，而是封装在 Hive 所依赖的 Jar 文件即 Java 代码中。\n\n用户接口\n包括 CLI、JDBC&#x2F;ODBC、WebGUI。其中，CLI(command line interface)为 shell 命令行；Hive 中的 Thrift 服务器允许外部客户端通过网络与 Hive 进行交互，类似于 JDBC 或 ODBC 协议。WebGUI 是通过浏览器访问 Hive。\nHive 提供了 Hive Shell、 ThriftServer 等服务进程向用户提供操作接口\n\n启动 hive\n\n启动元数据管理服务(必须启动，在 hive 目录下启动)\n\n前台启动 bin/hive --service metastore​​\n后台启动 nohup bin/hive --service metastore &gt;&gt; logs/metastore.log 2&gt;&amp;1 &amp;​​\n\n\n启动客户端\nHive Shell方式(可以直接写SQL) bin/hive​​\n​\n‍\n\n\n\n\nHive 使用语法数据库操作\n创建数据库\ncreate database if not exists myhive;​ 创建名为 myhive 的数据库\n\n查看数据库详细信息\ndesc database myhive\n​\n创建数据库并指定 hdfs 存储位置\ncreate database myhive2 location &#39;user/hive/myhive2&#39;;​\n\n删除一个空数据库，如果数据库下有数据表，那么就会报错\ndrop database myhive;​​\n\n强制删除数据库\ndrop database myhive2 cascade;​​\n\n内部表（CREATE TABLE table_name ……）未被 external 关键字修饰的即是内部表， 即普通表。 内部表又称管理表,内部表数据存储的位置由 hive.metastore.warehouse.dir 参数决定（默认：&#x2F;user&#x2F;hive&#x2F;warehouse），删除内部表会直接删除元数据（metadata）及存储数据，因此内部表不适合和其他工具共享数据。\n\n外部表（CREATE EXTERNAL TABLE table_name ……LOCATION……）被 external 关键字修饰的即是外部表， 即关联表。外部表是指表数据可以在任何位置，通过 LOCATION 关键字指定。 数据存储的不同也代表了这个表在理念是并不是 Hive 内部管理的，而是可以随意临时链接到外部数据上的。所以，在删除外部表的时候， 仅仅是删除元数据（表的信息），不会删除数据本身。\n\n先创建外部表，然后移动数据刀 LOCATION 目录\n\n检查 hadoop fs -ls /tmp​，确认不存在 /tmp/test_ext1 ​目录\n创建外部表：create external table test_ext1(id int,name string) row format delimited fields terminated by &#39;\\t&#39; location &#39;/tmp/test_ext1&#39;;​\nselect * from test_ext1 ​空结果，无数据\n上传数据:hadoop fs -put test_external.txt /tmp/test_ext1/​\nselect * from test_ext1​，即可看到数据\n\n\n先存在数据，后创建外部表\n\nhadoop fs -mkdir /tmp/test_ext2​\nhadoop fs -put test_external.txt /tmp/test_ext2/​\ncreate external table test_ext2(id int name string) row format delimited fields terminated by &#39;\\t&#39; location &#39;/tmp/test_ext2&#39;;​\nselect * from test_ext2;​\n\n\n\n数据在 HDFS 中以明文形式存在\n​\n自定义分隔符\ncreate table myhive.stu2(id int,name string)row format delimited fields terminated by &#39;\\t&#39;;​\n\n内外部表转换\n\n内转外\nalter table stu set tblproperties(&#39;EXTERNAL&#39;=&#39;TRUE&#39;);​\n\n外转内\nalter table stu set tblproperties(&#39;EXTERNAL&#39;=&#39;FALSE&#39;);​\n\n\n\nhive 表数据导出 -insert overwrite 方式\n\n语法 insert overwrite [local] directory &#39;path&#39; select_statement1 FROM from_statement;​\n\n将查询的结果导出到本地-使用默认分隔符\ninsert overwrite local diretory &#39;/home/hadoop/export1&#39; select  * from myhive.test_load;​\n\n将查询的结果导出到本地-指定分隔符\ninsert overwrite local directory &#39;/home/hadoop/export2&#39; row format delimited fields terminated by &#39;\\t&#39; select * from myhive.test_load;​\n\n将查询的结果导出到 HDFS 上(不带 local 关键字)\ninsert overwrite directory &#39;/tmp/export&#39; row format delimited fields terminated by &#39;\\t&#39; select * from myhive.test_load;​\n\n\n\n\n分区表可以把大的文件切割划分成一个个的小的文件，每次操作一个小的文件就会很容易\n​​​\n\n为什么不可以用 load data ​必须用 insert select ​插入数据\n问题在于:如何将数据划分，划分的规则是什么?\n数据的划分是基于分桶列的值进行 hash 取模来决定的，由于 load data ​不会触发 MapReduce ​也就是没有计算过程，无法执行 Hash 算法，只是简单的移动数据而已，所以无法用于分桶表数据插入。\n\n性能提升\n分区表:在指定分区列的前提下，减少被操作的数据量，从而提示性能。\n分桶表:基于分桶列的特定操作，如:过滤、JOIN、分组、均可带来性能提升\n\n\n修改表操作\n表重命名\nalter table old_table_name rename to new_table_name;\n\n修改表属性值\nalter table table_name SET TBLPROPERTIES table_properties;# table_properties:(property_name = property_value,property_name = property_value,...)# ALTER TABLE table_name SET TBLPROPERTIES(&quot;EXTERNAL&quot;=&quot;TRUE&quot;);  修改内外部表属性# ALTER TABLE table_name SET TBLPROPERTIES (&#x27;comment&#x27; = new_comment); 修改表注释\n\n添加分区\n新分区添加了但是空的没数据，需要手动添加或上传数据文件\nalter table tablename add partition (month=&#x27;202002&#x27;);\n修改分区(修改元数据记录，HDFS 的实体文件夹不会改名但是在元数据中是改名了的,内部表会改，外部表不会改)\nalter table tablename partition (month = &#x27;202003&#x27;) rename to partition (month=&#x27;202202&#x27;);\n删除分区(对于内部表而言删除元数据，数据本身还在，外部表则不在）\nalter table tablename drop partiton (month=&#x27;202104&#x27;);\n\n添加列\nalter table table_name add columns (v1 int,v2 string);\n修改列名\nalter table test_change change v1 v1newname int;\n删除表\ndrop table tablename;\n清空表\ntruncate table tablename;\n\nps:只可以清空内部表\n\n\n数据查询SELECT 基本查询\n查询所有\nselect * from orderdb.orders;\n查询单列\nselect orderid,totalmoney,username,useraddres,paytime from orderdb.orders;\n\n查询数据量\nselect count(*) from orderdb.orders;\n\n过滤广东省订单\nselect * from orderdb.orders where useraddress like &#x27;%广东%&#x27;;\n\n找出广东省单笔营业额最大的订单\nselect * from orderdb.orders where useraddress like &#x27;%广东%&#x27; order by totalmoney desc limit 1;# desc 降序，limit 1 最大的一个\n\n分组、聚合\n统计未支付、已支付各自的人数\nselect ispay,count(*) as cnt from orderdb.orders group by ispay;\n\n在已付款订单中，统计每个用户最高的一笔消费金额\nselect userid,MAX(totalmoney) as max_money from orderdb.orders where ispay = 1 group by userid;\n\n统计每个用户的平均订单消费额\nselect userid,avg(totalmoney) as avg_money from orderdb.orders group by userid having avg_money &gt; 10000;# having 表示筛选，与where不同;where在分组前进行筛选的,而having是在分组后进行筛选\n\nJOIN\nJOIN 订单表和用户表，找出用户名\n# 内关联，目的是获取来自 orders 表的订单编号 (orderid)、用户ID (userid)，以及与之相关联的用户表中的用户名 (username)。select o.orderid,o.userid,u.username,o.totalmoney,o.useraddress,o.paytime from orderdb.orders o join orderdb.users u on o.userid = u.userid;\n左外关联，订单表和用户表，找出用户名\n左连接 (​*LEFT JOIN*​) 会返回左边表（​*orders*​）中所有的记录，同时匹配右边表（​*users*​）中相应条件的记录。如果右边表中没有匹配的记录，那么将会返回 ​​*NULL*​​ 值。\nselect o.orderid,o.userid,u.username,o.totalmoney,o.useraddres,o.paytime from orderdb.orders o left join orderdb.users u on o.userid = u.userid;\n\nRLIKErelike 关键字，可以供用户使用正则和数据进行匹配\n# 查找广东省的数据select * from orderdb.orders where useraddress rlike &#x27;.*广东.*&#x27;;# 查找用户地址是：xx省 xx市 xx区的数据select * from orderdb.orders where useraddress rlike &#x27;..省 ..市 ..区&#x27;;# 查找用户姓为张、王、邓select * from orderdb.orders where username rlike &#x27;[张王邓]\\\\S+&#x27;;# 查找手机号符合:188****0*** 规则select * from orderdb.orders where userphone rlike &#x27;188\\\\S&#123;4&#125;0\\\\S&#123;3&#125;&#x27;;\n\nUNION 联合用于将多个 select ​语句的结果组合成单个结果集。每个 select ​语句返回的列的数量和名称必须相同。否则将引发架构错误。\n基础语法:\n\nselect …\nunion [all]\nselect …\n\n# 联合两个查询结果集select * from course where t_id = &#x27;周杰伦&#x27;;\tunionselect * from course where t_id = &#x27;王力宏&#x27;;\n\nunion 默认有去重功能:\n# 直接联合两个同样的查询结果select * from courseunionselcet * from course;# 不需要去重效果selcet * from course\tunion allselect * from course;\n\n\n其他写法\n# union写在from中selcet t_id,count(*) from(\tselect t_id from myhive.course where t_id =&#x27;周杰伦&#x27;\t\tunion all\tselect t_id from myhive.course where t_id = &#x27;王力宏&#x27;) as u group by t_id;# 用于insert selcet中create table myhive.course2 like myhive.course;insert overwrite table myhive.course2\tselcet * from myhive.course\t\tunion all\tselcet * from myhive.course;\n\n抽样操作在大体量的数据环境下,对于表的一个简单 select * 都会非常慢，哪怕看很少的数据都会走 MapReduce 流程，Hive 提供的快速抽样的语法，可以快速从大表中随机抽取一些数据供用户查看\nTABLESAMPLE 函数\n语法 1，基于随机分桶抽样:\nselect ... from tb1 tablesample(bucket x out of y on (colname | rand())​\n\ny 表示将表数据随机划分成 y 粉(y 个桶）\nx 表示从 y 里面随机抽取 x 份数据作为取样\ncolname 表示随机的依据基于某个列的值\nrand()表示随机的依据基于整行\n\n示例:\n注意:\n1.使用 colname 作为随机依据，则其它条件不变下，每次抽样结果一致；\n2.使用 rand()作为随机依据，每次抽样结果都不同\nselect username,orderid,totalmoney from orderdb.orders tableample(bucket 1out of 10 on username);select * from orderdb.orders tableample(bucket 1 out of 10 on rand());\n\n语法 2，基于数据块抽样\nselect ... from tb1 tableample(num rows | num percent | num(K|M|G));​\n\nnum rows 表示抽样 num 条数据\nnum perfect 表示抽样 num 百分百比例的数据\nnum(K|M|G)表示抽取 num 大小的数据，单位可以是 K、M、G 表示 KB、MB、GB\n\n注意:\n使用这种语法抽样，条件不变的话，每一次抽样的结果都一致；即无法做到随机，只是按照数据顺序从前向后取。\n\n\nVirtual Columns 虚拟列虚拟列是 Hive 内置的可以在查询语句中使用的特殊标记，可以查询数据本身的详细参数。Hive 目前可用 3 个虚拟列:\n\nINPUT_FILE_NAME,显示数据行所在的具体文件\n\nBLOCK_OFFSET_INSIDE_FILE,显示数据行所在文件的偏移量\n\nROW_OFFSET_INSIDE_BLOCK,显示数据所在 HDFS 块的偏移量\n\n此虚拟列需要设置:SET hive.exec.rowoffset&#x3D;true 才可使用\n\n\n\n示例:\nselect orderId,userName,INPUT__FILE__NAME,BLOCK__OFFSET__INSIDE__FILE from orders;​\n​\n\nHive 函数函数分类分为两大类:内置函数(Built-in Functions)、用户自定义函数 UDF（User-Defined Functions):\n​\n\n官方文档\n\n查看函数列表\n使用 show function 查看当下可用的所有函数\n通过 describe function extended funcname 来查看函数的使用方式\n​\n\nMathmatical Functions 部分数学函数\n\n–取整函数:round 返回 double 类型的整数值部分（四舍五入）\nselect round(3.1415926);\n–指定精度取证函数:round(double a,int d)返回指定精度 d 的 double 类型\nselect round(3.1415926,4);\n–取随机数:rand()每次执行都不一样，返回一个 0-1 范围内的随机数\nselect rand();\n–指定种子取随机函数:rand(int seed)得到一个稳定的随机序列\nselect rand(3);\n–求数字的绝对值\nselect abs(-3);\n–得到 pi 值(小数点后 15 位精度)\nselect pi();\n\n\nCollection Functions 集合函数\n\n\n\nReturn Type\nName(Signature)\nDescription\n\n\n\nint\nsize(Map&lt;K.V&gt;)\n返回 map 类型的元素个数\n\n\nint\nsize(Array)\n返回 array 类型的元素个数\n\n\narray\nmap_keys(Map&lt;K.V&gt;)\n返回 map 内的全部 key（得到的是 array）\n\n\narray\nmap_values(Map&lt;K.V&gt;)\n返回 map 内的全部 value（得到的是 array）\n\n\nboolean\narray_contains(Array, value)\n如果 array 包含指定 value，返回 True\n\n\narray\nsort_array(Array)\n根据数组元素的自然顺序按升序对输入数组进行排序并返回它\n\n\n\nType Conversion Functions 类型转换函数\n\n\n\nReturn Type\nName(Signature)\nDescription\n\n\n\nbinary\nbinary(string\nbinary)\n\n\nExpected “&#x3D;” to follow “type”\ncast(expr as )\n将表达式 expr 的结果转换为给定类型。例如，cast(‘1’ as BIGINT） 会将字符串 ‘1’ 转换为整数表示。如果转换不成功，则返回 null。对于 cast(expr as boolean)，对于非空字符串将会返回 True\n\n\n\nDate Functions 日期函数 - 部分\n\n\n\nReturn Type\nName(Signature)\nDescription\n\n\n\ntimestamp\ncurrent_timestamp()\n返回当前时间戳。在同一个查询中对 current _ time 戳的所有调用都返回相同的值。\n\n\ndate\ncurrent_date\n返回当前日期。在同一个查询中对 current_date 戳的所有调用都返回相同的值。\n\n\n2.1.0 版本之前返回 string 现在版本返回 date\nto_date(string timestamp)\n时间戳转日期\n\n\nint\nyear(string date)quarter(date&#x2F;timestamp&#x2F;string)month(string date)day(string date)dayofmonth(date)hour(string date)minute(string date)second(string date)weekofyear(string date)\n得到给定时间的：年得到给定时间的：季度得到给定时间的：月得到给定时间的：日得到给定时间的：当前月份第几天得到给定时间的：小时得到给定时间的：分钟得到给定时间的：秒得到给定时间的：本年第几周\n\n\nint\ndatediff(string enddate, string startdate)\n返回 enddate 到 startdate 之间的天数\n\n\n2.1.0 版本之前返回 string 现在版本返回 date\ndate_add(date&#x2F;timestamp&#x2F;string startdate, tinyint&#x2F;smallint&#x2F;int days)date_sub(date&#x2F;timestamp&#x2F;string startdate, tinyint&#x2F;smallint&#x2F;int days)\n日期相加: date_add(‘2008-12-31’, 1) &#x3D; ‘2009-01-01’.日期相减: date_sub(‘2008-12-31’, 1) &#x3D; ‘2008-12-30’.\n\n\n\nCondition Functions 条件函数\n\n\n\nReturn Type\nName(Signature)\nDescription\n\n\n\nT\nif(boolean testCondition, T valueTrue, T valueFalseOrNull)\n如果 testCondition 为 true，则返回 valueTrue，否则返回 valueFalseOrNull。\n\n\nboolean\nisnull( a )\n如果 a 为 NULL，则返回 true，否则返回 false。\n\n\nboolean\nisnotnull ( a )\n如果 a 不为 NULL，则返回 true，否则返回 false。\n\n\nT\nnvl(T value, T default_value)\n如果 value 为 null，则返回 default_value，否则 value。\n\n\nT\nCOALESCE(T v1, T v2, …)\n返回第一个不是 NULL 的 v，如果所有 v 都是 NULL，则返回 NULL。\n\n\nT\nCASE a WHEN b THEN c [WHEN d THEN e]* [ELSE f] END\n当 a &#x3D; b 时，返回 c;  [当 a &#x3D; d 时，返回 e]*  ;否则返回 f。\n\n\nT\nCASE WHEN a THEN b [WHEN c THEN d]* [ELSE e] END\nWhen a &#x3D; true, returns b; when c &#x3D; true, returns d; else returns e.a 可以是表达式，如 1&#x3D;1\n\n\nT\nnullif( a, b )\n如果 a&#x3D;b，则返回 NULL;否则返回 a 。等价：CASE WHEN a &#x3D; b then NULL else a\n\n\nvoid\nassert_true(boolean condition)\n如果 boolean_condition 结果不为 True，则引发异常报错比如：select assert_true (2&lt;1).\n\n\n\nString Functions 字符串函数\n\n\n\nReturn Type\nName(Signature)\nDescription\n\n\n\nstring\nconcat(string\nbinary A, string\n\n\nstring\nconcat_ws(string SEP, string A, string B…)\n同 concat，但是可以自己定义字符串之间的分隔符（SEP）\n\n\nint\nlength(string A)\n字符串长度\n\n\nstring\nlower(string A)upper(string a)\n全部转小写全部转大写\n\n\nstring\ntrim(string A)\n返回从 A 的两端裁剪空格得到的字符串。例如，trim(‘ foobar ’)的结果是‘ foobar’\n\n\narray\nsplit(string str, string pat)\n按照 pat 分隔字符串，pat 是正则表达式\n\n\n\nData Masking Functions 数据脱敏函数 -部分\n\n\n\nReturn Type\nName(Signature)\nDescription\n\n\n\nstring\nmask_hash(string|char|varchar str)\n对字符串进行 hash 加密非字符串加密会得到 NULL\n\n\n\nMisc. Functions 其他函数 -部分\n\n\n\nReturn Type\nName(Signature)\nDescription\n\n\n\nint\nhash(a1[, a2…]))\n返回参数的 hash 数字\n\n\nstring\ncurrent_user()\n返回当前登录用户\n\n\nstring\ncurrent_database()\n返回当前选择的数据库\n\n\nstring\nversion()\n返回当前 hive 版本\n\n\nstring\nmd5(string&#x2F;binary)\n返回给定参数的 md5 值\n\n\n\n\n案例基于 hadoop 和 hive 实现聊天数据系统分析，构建聊天数据分析表\n需求\n统计今日总消息量\n统计今日每小时消息量、发送和接收用户数\n统计今日各地区发送消息数据量\n统计今日发送消息和接收消息的用户数\n统计今日发送消息最多的 top10 用户\n统计今日接收消息最多的 Top10 用户\n统计发送人的手机型号分布情况\n统计发送人的设备操作系统分布情况\n\n数据内容\n大小:30w 条数据\n\n列分割符：hive 默认分隔符’\\001’\n\n数据字典及样例数据\n\n\n\n消息时间\n发件人昵称\n发件人账号\n发件人性别\n发件人 IP\n发件人系统\n发件人手机型号\n发件人网络制式\n发件人 GPS\n收件人昵称\n收件人 IP\n收件人账号\n收件人系统\n收件人手机型号\n收件人网络制式\n收件人 GPS\n收件人性别\n消息类型\n双方距离\n消息\n\n\n\n2021-11-0115:11:33\n古博易\n14747877194\n男\n48.147.134.255\nAndroid 8.0\n小米 Redmi K30\n4G\n94.704577,36.247553\n莱优\n97.61.25.52\n17832829395\nIOS 10.0\nApple iPhone 10\n4G\n84.034145,41.423804\n女\nTEXT\n77.82KM\n天涯海角惆怅渡，牛郎织女隔天河。佛祖座前长顿首，只求共度一百年。\n\n\n\n\n建库建表# 创建数据库create database db_msg;# 切换数据库use db_msg;# 列举数据库show databases;# 如果表存在就删除drop table if exists db_msg.tb_msg_source;# 建表create table db_msg.tb_msg_source(msg_time string comment &quot;消息发送时间&quot;,sender_name string comment &quot;发送人昵称&quot;,sender_account string comment &quot;发送人账号&quot;,sender_sex string comment &quot;发送人性别&quot;,sender_ip string comment &quot;发送人ip地址&quot;,sender_os string comment &quot;发送人操作系统&quot;,sender_phonetype string comment &quot;发送人手机型号&quot;,sender_network string comment &quot;发送人网络类型&quot;,sender_gps string comment &quot;发送人的GPS定位&quot;,receiver_name string comment &quot;接收人昵称&quot;,receiver_ip string comment &quot;接收人IP&quot;,receiver_account string comment &quot;接收人账号&quot;,receiver_os string comment &quot;接收人操作系统&quot;,receiver_phonetype string comment &quot;接收人手机型号&quot;,receiver_network string comment &quot;接收人网络类型&quot;,receiver_gps string comment &quot;接收人的GPS定位&quot;,receiver_sex string comment &quot;接收人性别&quot;,msg_type string comment &quot;消息类型&quot;,distance string comment &quot;双方距离&quot;,message string comment &quot;消息内容&quot;);\n\n加载数据先上传文件到 linux 系统，再通过 load 加载数据到表\nload data local inpath &#x27;/home/hadoop/chat_data-30w.csv&#x27; overwrite into table tb_msg_source;# 验证结果select msg_time,sender_name,sender_ip,sender_phonetype,receiver_name,receiver_network from tb_msg_source limit 10;\n\n\n\n数据清洗问题\n\n当数据中有一些数据的字段为空，不是合法数据\n\n\n\nselect msg_time,sender_name,sender_gps from db_msg.tb_msg_source where length(sender_gps) =0 limit 10;\n\n\n\n\n2.需求中，需要统计每天，每小时的消息量，但是数据中没有天和小时字段，只有整体时间字段，不好处理\n\nselect msg_time from db_msg.tb_msg_source limit 10;\n\n‍\n\n3.需求中，需要对经度和维度构建地区的可视化地图，但是数据中 GPS 经纬度为一个字段，不好处理\n\nselect sender_gps from db_msg.tb_msg_source limit 10;\n\n解决方法需求：\n\n需求 1 对字段为空的不合法数据进行过滤\nwhere ​过滤 将为空的数据过滤掉\nwhere length(sender_gps)&gt;0​\n\n需求 2 通过时间字段构建聊天和小时字段\ndate hour ​函数 分别取时间和小时\ndate(msg_time),hour(msg_time)​\n\n需求 3 从 GPS 的经纬度中提取经度和纬度\nsplit ​函数 分割成数据再分别取前一二个\nsplit(sender_gps,&#39;,&#39;)[0];​\n\n需求 4 将 ETL 以后的结果保存到一张新的 Hive 表中\n\n\n实现:\n# 创建一个新表create table db_msg.tb_msg_etl(msg_time string comment &quot;消息发送时间&quot;,sender_name string comment &quot;发送人昵称&quot;,sender_account string comment &quot;发送人账号&quot;,sender_sex string comment &quot;发送人性别&quot;,sender_ip string comment &quot;发送人ip地址&quot;,sender_os string comment &quot;发送人操作系统&quot;,sender_phonetype string comment &quot;发送人手机型号&quot;,sender_network string comment &quot;发送人网络类型&quot;,sender_gps string comment &quot;发送人的GPS定位&quot;,receiver_name string comment &quot;接收人昵称&quot;,receiver_ip string comment &quot;接收人IP&quot;,receiver_account string comment &quot;接收人账号&quot;,receiver_os string comment &quot;接收人操作系统&quot;,receiver_phonetype string comment &quot;接收人手机型号&quot;,receiver_network string comment &quot;接收人网络类型&quot;,receiver_gps string comment &quot;接收人的GPS定位&quot;,receiver_sex string comment &quot;接收人性别&quot;,msg_type string comment &quot;消息类型&quot;,distance string comment &quot;双方距离&quot;,message string comment &quot;消息内容&quot;,msg_day string comment &quot;消息日&quot;,msg_hour string comment &quot;消息小时&quot;,sender_lng double comment &quot;经度&quot;,sender_lat double comment &quot;纬度&quot;);# 使用insert 将改好的需求插入到新表中insert overwrite table db_msg.tb_msg_tb1select    *,date(msg_time) as msg_day,hour(msg_time) as msg_hour,    split(sender_gps,&#x27;,&#x27;)[0] as sender_lng,    split(sender_gps,&#x27;,&#x27;)[1] as sender_latfrom db_msg.tb_msg_sourcewhere length(sender_gps) &gt; 0;\n\n完成清洗的新表(部分):\n\n\n\nETL 概念\n\nE,Extract,抽取\nT,Transform,转换\nL,Load,加载\n\n从 A 抽取数据(E)，进行数据转换过滤(T),将结果加载到 B(L),就是 ETL\n\n\n需求指标统计\n指标 1：统计今日消息总量\ncreate table db_msg.tb_rs_total_msg_cnt comment &#x27;每日消息总量&#x27; as select msg_day,count(*) as total_msg_cnt from db_msg.tb_msg_etl group by msg_day;\n\n‍\n​\n\n‍\n‍\n\n统计每小时消息量、发送和接收用户数\ncreate table db_msg.tb_rs_hour_msg_cnt comment &#x27;每小时消息量趋势&#x27; as     select msg_hour,count(*) as total_msg_cnt,count(distinct sender_account) as sender_user_cnt,count(distinct receiver_account) as receiver_user_cntfrom db_msg.tb_msg_etl group by msg_hour;# as：这表示接下来的部分是一个 SQL 查询的结果将会被插入到新表中,count(*)：统计所有行的数量，即总消息数。# group by msg_hour：这是一个分组操作，它将查询结果按照 msg_hour 字段进行分组，意味着所有具有相同 msg_hour 值的行会被聚合在一起，从而得到每个小时的统计数据。\n\n​\n\n‍\n\n需求 3：统计今日各地区发送消息总量\ncreate table db_msg.tb_rs_loc_cnt comment &#x27;每日各地区发送消息总量&#x27; as    select msg_day,sender_lng,sender_lat,count(*) as total_msg_cntfrom db_msg.tb_msg_etl group by msg_day, sender_lng, sender_lat;\n\n​\n\n需求 4：统计今日发送和接收用户人数\ncreate table db_msg.tb_rs_user_cnt comment &#x27;今日发送和接收用户人数&#x27; as    select msg_day,count(distinct sender_account) as sender_user_cnt,count(distinct receiver_account) as receiver_user_cnt           from db_msg.tb_msg_etl group by msg_day;# 这里注意要加distinct关键字，进行去重，每个人每天能发送多条消息\n\n​\n\n需求 5：统计发送消息条数最多的 Top10 用户\ncreate table db_msg.tb_rs_s_user_top10 comment &#x27;发送消息条数最多的top10用户&#x27; as     select sender_name,count(*) as sender_msg_cntfrom db_msg.tb_msg_etl group by sender_name order by sender_msg_cnt desc limit 10;# desc 表示降序排序，意味着数值越大的将会排在前面\n\n​\n\n需求 6：统计接收消息最多的 top10 用户\ncreate table db_msg.tb_rs_r_user_top10 comment &#x27;接收消息条数最多的top10用户&#x27; as    select receiver_name,count(*) as receiver_msg_cntfrom db_msg.tb_msg_etl group by receiver_name order by receiver_msg_cnt desc limit 10;\n\n​\n\n需求 7：统计发送人的手机型号分布\ncreate table db_msg.tb_rs_sender_phone comment &#x27;统计发送人的手机型号分布&#x27; as    select sender_phonetype,count(*) as cnt from db_msg.tb_msg_etl group by sender_phonetype;\n\n​\n\n需求 8：统计发送人的手机 os 分布\ncreate table db_msg.tb_rs_sender_phone comment &#x27;统计发送人的手机型号分布&#x27; as    select sender_phonetype,count(*) as cnt from db_msg.tb_msg_etl group by sender_phonetype;\n​\n\n可视化平台  使用finebi平台，注册好后使用官方提供的激活码，激活使用即可\n\nFineBI与Hive集成文档：https://help.fanruan.com/finebi/doc-view-301.html\n\n驱动配置\n\n如果使用FineBI连接Hive，读取Hive的数据表，需要在FineBI中添加Hive的驱动jar包\n将Hive的驱动jar包放入FineBI的lib(FineBI6.0\\webapps\\webroot\\WEB-INF\\lib)目录下\n\n\n插件安装\n\n我们自己放的Hive驱动包会与FineBI自带的驱动包产生冲突，导致FineBI无法识别我们自己的驱动包\n安装FineBI官方提供的驱动包隔离插件(fr-plugin-hive-driver-loader-3.0.zip)\n安装好后重启finalbi即可\n\n\n测试连接\n\n\n\n基于FineBI完成指标的可视化展现数据准备在BI界面，点击公共数据，新建个Hive的文件夹然后添加数据库表，选择db_msg的八项数据，点击确定；然后点击每个表，更新数据。\n(过程中可能会出现中文乱码情况，解决方法可见:https://www.cnblogs.com/qingyunzong/p/8724155.html 不过值得注意的是:之前创建的表中的中文依然会是问号)\n\n新建分析，点击我的分析-新建文件夹(hive数据分析)-新建分析主题\n\n\n在主题里面点击图标类型的123数字类型-&gt;将sender_user_cnt拖到文本框-&gt;修改文本-&gt;取消固定大小，并将sender_user_cnt修改为发送消息人数，并修改组件名，拖入仪表盘；\n\n\n\n同理添加其它组件\n","categories":["BigData"],"tags":["hadoop","hive","hdfs"]},{"title":"python利用列表实现队栈操作","url":"/2023/08/24/python%E5%88%A9%E7%94%A8%E5%88%97%E8%A1%A8%E5%AE%9E%E7%8E%B0%E9%98%9F%E6%A0%88%E6%93%8D%E4%BD%9C/","content":"使用列表实现队列和栈操作\n栈栈是一种后进先出的数据结构，在python中可以使用列表来实现一些相关操作\nstack = [] # 创建一个栈# 入栈操作stack.append(1)stack.append(2)stack.append(3)# 出栈操作num = stack.pop()print(&quot;出栈元素:&quot;,num)print(&quot;更新后的栈:&quot;,stack)\n\n\n\n\n程序输出出栈元素:3更新后的栈:[1,2]\n\n\n\n\n队列队列是一种先进先出的数据结构，同理也可以使用列表来实现\nqueue = [] # 创建一个队列# 入队操作queue.append(1)queue.append(2)queue.append(3)# 出队操作num = queue.pop(0) #注意这里和栈不一样print(&quot;出队元素:&quot;,num)print(&quot;更新后的队列:&quot;,queue)\n\n\n\n\n输出出队元素:1更新后的队列:[2,3]\n\n\n\n\n\n使用collections模块来实现队列\n队列使用collections模块中的deque类来实现队列操作。deque是一个双端队列，支持在队列两端进行插入和删除操作。\n# 导入deque类from collections import deque# 创建一个空队列queue = deque()# 入队queue.append(1)# 出队element = queue.popleft()\n\n","categories":["python"],"tags":["python语法","栈和队列"]},{"title":"计算机网络","url":"/2023/04/06/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/","content":"计网概述因特网服务提供者ISP\n因特网的组成\n边缘部分 由所有连接在因特网上的主机组成，这部分是用户直接使用和资源共享\n核心部分 由大量网络和连接这些网络的路由器组成，这部分是为边缘部分提供服务的\n\n三种交换方式\n电路交换\n\n\n电话交换机接通电话线的方式\n电路交换的三个步骤\n建立连接 (分配通信资源)\n通话(一直占用资源)\n释放连接(归还通信资源)\n\n\n当使用电路交换来传送计算机数据时，线路的传输效率往往很低\n\n分组交换发送方:构造分组发送分组路由器:缓存分组转发分组接收方:接受分组还原报文报文交换与分组类似，主要用于早期的电报通信网，现在较少使用  \n三者对比三者优缺点电路交换:\n\n优点\n通信时延小\n有序传输\n没有冲突\n适用范围广\n实时性强\n控制简单\n\n\n缺点\n建立连接时间长\n线路独占，使用效率低\n灵活性差\n难以规格化\n\n\n\n报文交换:\n\n优点\n无需建立连接\n动态分配线路\n提高线路可靠性\n提高线路利用率\n提供多目标服务\n\n\n缺点\n引起转发时延\n需要较大存储缓存空间\n需要传输额外的信息量\n\n\n\n分组交换:\n\n优点\n无需建立连接\n线路利用率高\n简化了储存管理\n加速传输\n减少出错概率和重发数据量\n\n\n缺点\n引起转发时延\n需要传输额外的信息量\n对于数据服务，存在失序，丢失或重复分组的问题，对于虚电路服务，存在呼叫建立，数据传输和虚电路释放三个过程\n\n\n\n计算机网络分类按交换技术分类\n电路交换网络\n报文交换网络\n分组交换网络\n\n按使用者分\n公用网\n专用网\n\n按传输介质分类\n有线网络\n无线网络\n\n按覆盖范围分类\n广域网 WAN\n域域网 WAN\n局域网 LAN\n个域网 PAN\n\n按拓扑结构分类\n总线型网络\n星形网络\n环形网络\n网状型网络\n\n计算机网络的性能指标速率速率:是指在计算机网络上的主机在数字信道上传送比特的速率，也称为比特率或数据率比特:计算机中数据量的单位常用数据量单位常用数据率单位\n带宽带宽:(在计算机网络中的意义)用来表示网络的通信线路所能传送数据的能力。一条通信线路的频带宽度越宽，其所传输的最高数据率也越高单位:b/s(kb/s,Mb/s,Gb/s,Tb/s)\n吞吐量吞吐量：表示单位时间内通过某个网络的数据量吞吐量受网络带宽或额定速率的限制\n时延\n发送时延分组长度发送速率\n传播时延  信道长度电磁波传播速率电磁波传播速率:1.自由空间 3x10^8m/s 2.铜线 2.3x10^8m/s 3.光纤 2.0x10^8m/s\n处理时延一般不方便计算\n\n时延带宽积时延带宽积=传播时延X带宽\n往返时间 RTT\n利用率$$利用率=\\left{信道利用率表示某信道有百分之几的时间是利用的网络利用率全网络的信道利用率的加权平均\\right.$$\n丢包率丢包率是指在一定时间范围内，传输过程中丢失的分组数量与总分组数量的比率分组丢失主要有两种情况    1. 分组在传输过程中出现误码被结点丢弃    2. 分组到达一台队列已满的分组交换机时被丢弃；在通信量较大时就可能造成网络拥塞丢包率也反应了网络的拥塞情况\n\n无拥塞时路径丢包率为0\n轻度拥塞时路径丢包率为1%-4%\n严重拥塞时路劲丢包率为5%-15%\n\n","categories":["笔记"],"tags":["计算机网络"]},{"title":"力扣<py>","url":"/2023/06/04/%E5%8A%9B%E6%89%A3-py/","content":"977.有序数的平方\n使用python推导式+内置sort函数表达式:[表达式 for 变量 in 列表] or [表达式 for 变量 in 列表 条件] 使用推导式计算平方后的数据，再调用sort()方法完成排序\n class Solution:    def sortedSquares(self, nums: List[int]) -&gt; List[int]:        res=[num**2 for num in nums]        res.sort()        return res\n\n采用快排超出时间限制  \n  class Solution:def Partion(self,res,low,high):    i=(low - 1)    pivot=res[high]    for j in range(low,high):        if res[j]&lt;=pivot:            i=i+1            res[i],res[j]=res[j],res[i] # 小于pivot的往后移动    res[i+1],res[high]=res[high],res[i+1] #把pivot放入合适位置    return (i+1)def Qsort(self,res,low,high):    if low &lt;high :        pi = self.Partion(res,low,high)        self.Qsort(res,low,pi-1)        self.Qsort(res,pi+1,high)def sortedSquares(self, nums: List[int]) -&gt; List[int]:    res=[num**2 for num in nums]    self.Qsort(res,0,len(nums)-1)    return res\n\n双指针 采用双指针，left和right。先建立一个新的数组其值是nums数组平方后的，然后left指向第一个，right指向最后一个，在定义s为新指针，从当left&lt;&#x3D;right时，双向遍历，并将其值大的一个放入原数组nums的最后，当一次遍历完时，nums数组也完成了从小到大的排序。\nclass Solution: def sortedSquares(self, nums: List[int]) -&gt; List[int]:     res=[num**2 for num in nums]     left=0     right=len(nums)-1     s=right #s是新指针，从后往前排     # 大的往后排     while left&lt;=right:         if res[left]&lt;=res[right]:             nums[s]=res[right]             s-=1             right-=1         else:             nums[s]=res[left]             s-=1             left+=1     return nums\n\n27.移除元素\n一次遍历把不等于val的值放入数组中\nclass Solution: def removeElement(self, nums: List[int], val: int) -&gt; int:     if (nums == None and len(nums)==0):         return 0     s=0 # s的值代表数组的下标     for f in range(0,len(nums)):         if (nums[f]!=val):             nums[s]=nums[f]             s+=1     return s\n\n59.螺旋矩阵||\n采用四个循环每次循环解决一个方向 初始化数组后先从左边开始解决，矩阵是n*n的大小，将矩阵第一个默认设置为1，设置一个变量cnt&#x3D;2，从第二个开始依次填充，当下一位位置未被填充时填充cnt，当左边达到边界并且下一个是未被填充时，开始像下填充，同理，当向下填充达到边界且下一个未被填充后向左，再向上，当cnt&#x3D;&#x3D;n^2时完成螺旋矩阵\n class Solution:def generateMatrix(self, n: int) -&gt; List[List[int]]:    nums = [[0] * n for _ in range(n)] # 初始化数组 n行n列    i=0    j=0    cnt = 2    nums[0][0]=1    while cnt &lt;= n*n:        # left        while j&lt;n-1 and nums[i][j+1]==0:            j +=1            nums[i][j]=cnt            cnt +=1        # down        while i&lt;n-1 and nums[i+1][j]==0:            i+=1            nums[i][j]=cnt            cnt+=1        # left        while j&gt;0 and nums[i][j-1]==0:            j-=1            nums[i][j]=cnt            cnt +=1        # up        while i&gt;0 and nums[i-1][j]==0:            i-=1            nums[i][j]=cnt            cnt +=1    return nums\n\n1.两数之和\n双重循环利用两个for循环完成查找\n class Solution:def twoSum(self, nums: List[int], target: int) -&gt; List[int]:    ans=[0] *2     for i in range(len(nums)):        for j in range(i + 1, len(nums)):            # 如果两个数相加等于target记录其下标             if nums[i] + nums[j] == target:                ans[0] = i                ans[1] = j    return ans\n\n字典+枚举 从评论区看到的方法，利用字典的键值配对，和target0-val来找另一个val的方式，很巧妙 字典+枚举的方式可以优化算法使得时间复杂度＜o(n^2) 先使用dict()函数创建一个空的records字典，然后再用enumerate()函数将nums变成枚举类型，遍历nums，判断target - val是否在字典records里面(若数组中有两个数val加起来等于target，那么target-val的值也一定在数组中)，若不在则将val和其下标index记录字典，若在字典中则返回target-val值的下标和val的下标，即为所求，代码如下：\n class Solution:def twoSum(self, nums: List[int], target: int) -&gt; List[int]:    records=dict()    for index,val in enumerate(nums):        if target - val not in records:            records[val]=index # 记录当前val的索引值到字典中        else:             return [records[target-val],index]\n\n重新排序+双指针 利用sorted()函数将数组重新排序，注意:不能直接排序nums数组，这这样会改变其原有的索引值，可以用下面两种方法来排序:1.利用numerate()函数将nums转换成元组，按照值来排序，这样不会改变其原有元素的索引值。2.直接排序原数组的下标，将待排序的数组利用range(len(nums))生产一个0~len(nums)-1的序列，按照nums数组的值来进行排序，这样就能得到一个按照值来排序的索引数组。排好序后，利用双指针head和tail一个从前往后一个从后往前，依次遍历判断和与target的关系，两种方式的代码如下: 第一种：\n class Solution:def twoSum(self, nums: List[int], target: int) -&gt; List[int]:    sorted_list = sorted(enumerate(nums),key=lambda x: x[1]) # 排好序的元组    head = 0    tail = len(nums) - 1    res = sorted_list[head][1] + sorted_list[tail][1] #用来判断与target的关系    while res != target:        if res &gt; target: # 和比target大说明tail的值大了，tail向前移动            tail -= 1        if res &lt; target: # 和比target小说明head的值小了，head向后移动            head += 1        res = sorted_list[head][1] + sorted_list[tail][1] # 每次移动完成后更新res    return [sorted_list[head][0], sorted_list[tail][0]]\n\n 第二种:\n class Solution:def twoSum(self, nums: List[int], target: int) -&gt; List[int]:    sorted_index=sorted(range(len(nums)),key = lambda x:nums[x]) # 排序nums，返回的索引序列    head = 0    tail = len(nums) - 1    res = nums[sorted_index[head]] + nums[sorted_index[tail]] # sorted_index[head]其值对于nums索引位置    while res != target:        if res &gt; target:            tail  -= 1        if res &lt; target:            head += 1        res = nums[sorted_index[head]] + nums[sorted_index[tail]]    return [sorted_index[head],sorted_index[tail]]\n\n34.在排序数组中寻找元素的第一个和最后一个位置\n类二分查找类似于二分查找，找到第一个等于target的位置，记录后，再继续找到大于target的第一个位置记录，则这区间为所求，关键在于:边界控制条件。由于边界控制不好，导致代码无法运行，看了评论区代码后，依照写了两个函数searchleft和searchright分别查找左边界和有边界；后面看到另一位大神的代码后，发现可以更加简洁，只需一个函数searchleft找到左边界后，右边界依旧使用searchleft，查找值为target+1找到后再将结果-1这样就找到了target的右边界，非常巧妙！\n class Solution:def searchRange(self, nums: List[int], target: int) -&gt; List[int]:    # 找到左边界    def SearchLeft(nums,target):        low=0        high=len(nums)-1        while low&lt;=high:            mid = (low+high)//2            # 判断条件必须加等号，防止漏掉元素            if nums[mid]&gt;=target:                high=mid-1            else:                low=mid+1        return low    # 找右边界    # def SearchRight(nums,target):    #     low=0    #     high=len(nums)-1    #     while low&lt;=high:    #         mid = (low+high)//2    #         if nums[mid]&gt;target:    #             high=mid-1    #         else:    #             low=mid+1    #     return high    # leftindex=SearchLeft(nums,target)    # rightindex=SearchRight(nums,target)    # searchleft找到的是第一个&gt;target+1的数，数组是有序的；-1后即为target的右边界值    leftindex=SearchLeft(nums,target)    rightindex = SearchLeft(nums,target+1)-1    if leftindex&lt;=rightindex:        return [leftindex,rightindex]    else:        return [-1,-1]\n\n242.有效字母的异位词\n用数组记录出现字符串的个数，在比较数组定义一个数组arr初始化为0.用来记录s中字符出现的个数，利用ord()函数，将s中每个字符转换为Unicode值，再在数组相应位置+1操作，遍历s，这样就记录了s中所有字符出现的个数。同样再遍历t，不过这次数组再相应位置进行-1操作，如果两个字符串所含字符个数是相等的，那么arr数组应该为0，若不为0则不等，代码:\nclass Solution: def isAnagram(self, s: str, t: str) -&gt; bool:     # 判断s 和 t 的长度是否相等     if (len(s)==len(t)):         arr = [0] * 26 # 初始化数组用来记录 s 中出现字符的个数         for i in s:             arr[ord(i)-ord(&#x27;a&#x27;)]+=1         for j in t:             arr[ord(j)-ord(&#x27;a&#x27;)]-=1 # 若出现相同的字符贼减一         # 判断arr数组是否为空         for i in range(26):             if arr[i]!=0:                 return False         return True     else:         return False\n\ndefaultdictdefaultdict是python中的一个内置字典子类，重载了一个方法来实现默认值的设定。在创建defaultdict对象，需要提供一个参数作为默认值或者一个函数用来生成默认值。\nclass Solution:def isAnagram(self, s: str, t: str) -&gt; bool:    from collections import defaultdict    sdict=defaultdict(int)    tdict=defaultdict(int)    for i in s:        sdict[i]+=1    for j in t:        tdict[j]+=1    if sdict==tdict:        return True    else:        return False\n\n202.快乐数\n转为字符串计算平方，然后判断是否循环我一开始是想先把给的数字每次队10取余然后平方加起来，后面发现python可以更简单的实现这一步，就是把给的n转化为字符串然后遍历字符串将每次遍历的字符转换为int在平方相加，然后再定义一个列表，大小为n，将每次替换后的结果存入这个列表中，若此后的遍历出现了和列表中一样的数，那么这个数就不是快乐数，若循坏到1则为快乐数，代码如下:\n class Solution:    def Caculate_Pow(self,num:int) -&gt; int:        n = 0 #用来返回的值        # 将n变为字符串遍历再变为int求平方和        for i in str(num):            n += int(i)**2        return n    def isHappy(self,n:int) -&gt; bool:        ret = [n] #设置一个n大小的列表用来判断是否出现了循环        while n!=1:            n=Caculate_Pow(n)            if n in ret:                return False            else:                ret.append(n) # 若未出现再ret中且不等于1时，将其加入到ret中        return True\n\n454. 四数相加 II\n哈希表+分治这题没思路，做不出，听了老师的讲解，由题所给条件nums1[i] + nums2[j] + nums3[k] + nums4[l] &#x3D;&#x3D; 0 –&gt; nums1[i] + nums2[j] &#x3D; - nums3[k] - nums4[j] 所以我们可以分别处理nums1 2 和nums3 4 并且定义两个字典dic_A 和 dic_B 分别记录两个数组组合的值和次数。再判断dic_A和中与dic_B相同的值，并且将两者出现的次数相乘，因为是组合问题所以应该是相乘而不是相加代码示例如下:\nclass Solution:def fourSumCount(self, nums1: List[int], nums2: List[int], nums3: List[int], nums4: List[int]) -&gt; int:    dic_A = &#123;&#125;    dic_B = &#123;&#125;    res = 0    # 先求两个的交集    #nums1[i] + nums2[j]  == - nums3[k] - nums4[l]    for i in nums1:        for j in nums2:            sum_ij = i + j            dic_A[sum_ij] = dic_A.get(sum_ij,0) + 1 # 记录sum_ij出现的次数        for k in nums3:        for l in nums4:            sum_kl = -k-l            dic_B[sum_kl] = dic_B.get(sum_kl,0) + 1        # 如果dic_A和dic_B有交集，他们次数乘积累加后为所求，注意这是组合问题，不能相加    for item in dic_A.items():        if item[0] in dic_B:            res += item[1] * dic_B[item[0]]    return res\n\n344. 反转字符串\n双指针原地逆转设置双指针i,j一个指向头部，一个指向尾部，再用tmp当中间变量交换，当i&gt;&#x3D;j时完成交换。\nclass Solution:def reverseString(self, s: List[str]) -&gt; None:    &quot;&quot;&quot;    Do not return anything, modify s in-place instead.    &quot;&quot;&quot;    i = 0    j = len(s) - 1    while i&lt;=j:        tmp = s[j]        s[j] = s[i]        s[i] = tmp        i += 1        j -= 1\n\n876. 链表的中间结点\n快慢指针设置快慢指针，慢的为slow，快的为fast，慢的一次走一步，快的一次走两步，当fast走到末尾时，slow处于中间位置\n# Definition for singly-linked list.# class ListNode:#     def __init__(self, val=0, next=None):#         self.val = val#         self.next = nextclass Solution:    def middleNode(self, head: Optional[ListNode]) -&gt; Optional[ListNode]:        if head == None:            return None        slow = head        fast = head        while fast and fast.next:            slow = slow.next            fast = fast.next.next        return slow\n\n206.反转链表\n双指针+临时变量记录我这里是重新返回了一个新的链表，按照头插法遍历原链表，就完成了链表的反转\n# Definition for singly-linked list.# class ListNode:#     def __init__(self, val=0, next=None):#         self.val = val#         self.next = nextclass Solution:    def reverseList(self, head: Optional[ListNode]) -&gt; Optional[ListNode]:        # 链表为空直接返回        if head == None:            return head        # i指针遍历head链表        i = head        NewNode = None        # j指针指向新的链表        j = NewNode        # 头插法实现逆转        while i!= None:            tmp = i.next            i.next = j            j = i            i = tmp        return j        \n\n160. 相交链表\n先判断长度，然后从位置相同的结点同时遍历，找相同的结点先计算后两个链表的长度，然后利用一个变量计算差值，让长的那个链表与短的那个链表同时向后遍历，找到相同的结点\n# Definition for singly-linked list.# class ListNode:#     def __init__(self, x):#         self.val = x#         self.next = Noneclass Solution:    # 计算链表的长度    def get_length(self,head:ListNode) -&gt; int:        count = 0        i = head        while i:            count += 1            i = i.next        return count    def getIntersectionNode(self, headA: ListNode, headB: ListNode) -&gt; Optional[ListNode]:        m = self.get_length(headA)        n = self.get_length(headB)        node_headA = headA        node_headB = headB        判断哪个链表长，记录差值，然后让长的链表先走，保证两个链表能在同一个位置向后遍历        if m &gt; n:            cha = m - n            i_headA = 0            while i_headA &lt; cha:                node_headA=node_headA.next                i_headA += 1        else:            cha = n - m            i_headB = 0            while i_headB &lt; cha:                node_headB=node_headB.next                i_headB += 1        # 如果遍历到相同的结点返回        while node_headA and node_headB:            if node_headA == node_headB:                return node_headA            node_headA = node_headA.next            node_headB = node_headB.next        return None\n\n20.有效的括号一开始我是想记录每个字符出现的次数然后判断是否为奇数，当然通过不来，因为没要考虑到顺序问题，”([)]”这个测试用例就会失败\n\n使用replace()替换字符设置判断条件，遇到’{}’,’[]’,’()’,直接替换为空字符\nclass Solution:def isValid(self, s: str) -&gt; bool:    while &#x27;&#123;&#125;&#x27; in s or &#x27;()&#x27; in s or &#x27;[]&#x27; in s:        s=s.replace(&#x27;&#123;&#125;&#x27;,&#x27;&#x27;)        s=s.replace(&#x27;[]&#x27;,&#x27;&#x27;)        s=s.replace(&#x27;()&#x27;,&#x27;&#x27;)    return s==&#x27;&#x27;\n\n哈希+栈K神解答;先创建一个包含所需字符的字典dic，然后再定义一个stack数组用来模拟栈操作，遍历s看字符是否存在于dic中，若存在入栈(查找左括号的过程),当遍历到不是左括号的时候出栈，看出栈的元素与当前元素是否匹配，若不匹配直接返回False，需要注意的是，stack初始时应该添加一个变量进去，同理在dic中添加一个不属于s的字符，最后判断stack长度为1时，返回True\n class Solution:def isValid(self, s: str) -&gt; bool:    dic = &#123;&#x27;[&#x27;:&#x27;]&#x27;,&#x27;(&#x27;:&#x27;)&#x27;,&#x27;&#123;&#x27;:&#x27;&#125;&#x27;,&#x27;a&#x27;:&#x27;a&#x27;&#125;    stack = [&#x27;a&#x27;]    for i in s:        if i in dic:            stack.append(i)        elif dic[stack.pop()] != i:            return False    return len(stack) == 1\n\n496. 下一个更大元素 I\n由题可知nums2包含了nums1，因此可以用index查找nums1中数在nums2中的下标，然后再判断从该处起是否有大于nums1的数，若有则赋值到nums1的相应位置，若没有则将nums1相应位置设置为-1\nclass Solution:def nextGreaterElement(self, nums1: List[int], nums2: List[int]) -&gt; List[int]:    for i in range(len(nums1)):        index = nums2.index(nums1[i]) + 1 # 查找nums1中的值再nums2中的位置，加一代表下一位        flag = 0 # flag用来判断nums2中是否有符合条件的数        while index &lt; len(nums2):            if nums2[index] &gt; nums1[i]: # nums2的右边有大于nums1的数                nums1[i] = nums2[index] # 将第一个大于的数赋值到nums1相应的位置                flag = 1                break            index += 1        if flag == 0:            nums1[i] = -1    return nums1\n\n字典+双循环(同学解答) 其实这种方式和我自己用index方式找下标本质上是一样的，不过这是用到了双循环，从nums2中去找第一个大于nums当前位置上的数。具体实现是:\n\n\n\n1.先初始化结果数组为-1 长度为nums1的长度 2.然后将nums2中的数字和下标存入到字典当中 3.遍历nums1，同时遍历nums2从nums1当前值在nums2中的下标开始往后找，若找到一个比nums1此时位置上的数大的数后将其赋值到结果数组\n\n\n\n class Solution:def nextGreaterElement(self, nums1: List[int], nums2: List[int]) -&gt; List[int]:    ans = [-1] * len(nums1)    dic = &#123;&#125;    # 将nums2的数字和下标存入字典    for item in range(len(nums2)):        dic[nums2[item]] = item    # 遍历nums1 在nnums2中从nums1当前数字的下标往后找若有大于nums1的就赋值放入ans中    for i in range(len(nums1)):        for j in range(dic[nums1[i]],len(nums2)):            if nums2[j] &gt; nums1[i]:                ans[i] = nums2[j]                break # 找到一个就结束当前循环    return ans\n\n字典+栈(同学解答) 这个方法采用的是将nums2中存在后一个数大于前一个数的存入到字典中，key为待查找数，value为向后第一个大于key的数。当遍历完nums2后，从nums1开始遍历，并查找dic中是否存在该数，存在将其放到待放回数组中，具体代码实现:\n class Solution:def nextGreaterElement(self, nums1: List[int], nums2: List[int]) -&gt; List[int]:    stack = []    dic = &#123;&#125;    res = []    for i in range(len(nums2)):        # 当栈不为空，并且nums2[i]大于栈顶元素时，更新字典        while stack and stack[-1]&lt;nums2[i]:            dic[stack.pop()] = nums2[i]        stack.append(nums2[i]) # 更新栈顶、    # 在nums2中查找nums1    for i in nums1:        res.append(dic.get(i,-1))    return res\n\n150. 逆波兰表达式求值\n使用栈来计算定义一个栈，设置条件判断，是运算符号就出栈，计算后后再入栈，这里注意的是在计算正数与负数相除时，要使用int进行取整，不能用 &#x2F;&#x2F; 进行整除。再进行减法和除法运算时要先用一个临时变量记录出栈的数据，再用用出栈的数减去或者除以这个临时变量，遍历tokens计算得到最后的结果\nclass Solution:def evalRPN(self, tokens: List[str]) -&gt; int:    if len(tokens) == 1:        return int(tokens[0])    stack = []    res = 0    for i in tokens:        if i != &#x27;+&#x27; and i != &#x27;-&#x27; and i != &#x27;/&#x27; and i != &#x27;*&#x27;:            stack.append(int(i))        elif i == &#x27;+&#x27;:            res = stack.pop() + stack.pop()            stack.append(res)        elif i == &#x27;-&#x27;:            temp = stack.pop() # 减数            res = stack.pop() - temp            stack.append(res)        elif i == &#x27;*&#x27;:            res = stack.pop() * stack.pop()            stack.append(res)        elif i == &#x27;/&#x27;:            tmp = stack.pop() # 除数            res = int(stack.pop()/tmp)            stack.append(res)    return res\n\nLC官方解答(栈)\nclass Solution:    def evalRPN(self, tokens: List[str]) -&gt; int:        op_to_binary_fn = &#123;            &quot;+&quot;:add,            &quot;-&quot;:sub,            &quot;*&quot;:mul,            &quot;/&quot;:lambda x,y:int(x/y),        &#125;        stack = list() # 定义栈        for token in tokens:            try:                num = int(token) # 不是操作符            except ValueError: # 遇到操作符执行相应的操作                num2 = stack.pop()                num1 = stack.pop()                num = op_to_binary_fn[token](num1,num2) # 注意num1和mum2的位置，先出栈的作为除数            finally:                stack.append(num)        return stack[0] # 最后的运算结果\n\n347. 前 K 个高频元素\n排序+字典先将原数组排好序，再统计每个数字出现的次并将其存入到字典dict_nums中，再将dict_nums按照出现的次数来进行排序，最后将前k个赋值到list_r中即可\n\n\n\n代码如下\n\n\n\nclass Solution:def topKFrequent(self, nums: List[int], k: int) -&gt; List[int]:    # 用来返回结果的列表    list_r = []    # 存放每个数字及出现的次数    dict_nums = &#123;&#125;    # 记录每个数字出现的次数    cnt = 0    # 先排序    nums = sorted(nums)    # 将每个数字出现的次数记录到dict_nums    tmp = nums[0]    i = 0    while i &lt; len(nums):        if nums[i] == tmp:            cnt += 1        if nums[i] != tmp:            dict_nums[nums[i - 1]] = cnt            cnt = 1            tmp = nums[i]        i += 1    i = 0    # 记录最后一个数字的出现次数    dict_nums[nums[len(nums) - 1]] = cnt    # 如果原数组不止一个数的话，按照值的大小来排序字典    if len(dict_nums) &gt; 1:        new_dict = sorted(dict_nums.items(), key=lambda x: x[1],reverse=True)        new_dict = dict(new_dict)    else:        new_dict = dict_nums    # 将原数组中出现次数前k的赋值到list_r    for key, val in new_dict.items():        if i &lt; k:            list_r.append(int(key))            i += 1    return list_r\n\n\n\n\n简洁版代码\n\n\n\nclass Solution: def topKFrequent(self, nums: List[int], k: int) -&gt; List[int]:     dic = &#123;&#125;     for i in nums:         dic[i] = dic.get(i,0) + 1     return([i for i,j in sorted(dic.items(),key=lambda x:x[1],reverse=True)][0:k])\n\n字典+堆先用字典记录每个数字及其出现次数，在将其出现次数都*-1，然后根据次数来建立小根堆，遍历小根堆，则为前k个高频元素\n\n\n\n代码如下\n\n\n\nclass Solution:def topKFrequent(self, nums: List[int], k: int) -&gt; List[int]:    dic = &#123;&#125;    res = [] # 用来返回结果    heap = [] # 建立小根堆    # 记录每个数字及其出现次数    for i in nums:        dic[i] = dic.get(i,0) + 1    # 将其转换成列表，并且将键值反转，并且-1*出现次数，以便后面建立小根堆    ls = [(-1 * value,key) for key,value in dic.items()]    # 建立小根堆    heapq.heapify(ls)    for i in range(k):        res.append(heapq.heappop(ls)[1])    return res\n\n278. 第一个错误的版本\n二分查找如果直接用for循环进行遍历的话，会超出时间复杂度，所以这里可以选择采用二分查找的方式，时间复杂度为O(log n),不过针对这个题目，不能直接套用二分查找，应作相应的改变，最后返回的值不是mid而是first，因为可能不止一个错误的版本，而题目要求返回第一个错误的版本，所以当isBadVersion(mid)为True时last = mid - 1查找左半部分是否还存在错误版本，反之则first = mid + 1,当first&gt;last时，first的位置就为第一个错误版本\n# The isBadVersion API is already defined for you.# def isBadVersion(version: int) -&gt; bool:class Solution:    def firstBadVersion(self, n: int) -&gt; int:        first = 1        last = n        while first &lt;= last:            mid = (first + last) // 2            if isBadVersion(mid):                last = mid -1 # 查找左半部分是否还存在错误版本            else:                first = mid + 1        return first # 当first&gt;last时first所在位置就是首个错误版本\n\n704. 二分查找\n二分查找非常典型的二分查找题目，直接使用二分查找的相关代码即可\nclass Solution: def search(self, nums: List[int], target: int) -&gt; int:     low = 0     high = len(nums) - 1     while low &lt;= high:         mid = (low + high) // 2         if nums[mid] == target:             return mid         elif nums[mid] &gt; target:             high = mid -1         else:             low = mid + 1     # 不存在返回-1     return -1\n\n240.搜索二维矩阵 II\n暴力二分法整个二维矩阵又可以看成是m个含有n个元素的一维数组组成，因此可以遍历真个m个数组，在遍历每一个数组的时候采用二分查找减少时间消耗，这样的时间复杂度为mlogn\nclass Solution:# 二分查找def func(self,nums,target):    low = 0 # 首    high = len(nums) - 1 # 尾    while low &lt;= high:        mid = (low + high ) // 2        if nums[mid] == target:            return True        elif nums[mid] &gt; target:            high = mid - 1        else:            low = mid + 1    return Falsedef searchMatrix(self, matrix: List[List[int]], target: int) -&gt; bool:    m = len(matrix) # 行数，表示又多少个一维数组    n = len(matrix[0]) # 列数，表示每个一维数组中所含元素的个数    # 遍历每个一个一维数组使用二分查找去查找    for i in range(m):        if self.func(matrix[i],target):            return True    return False\n\n分治法整个二维数组左下角的第一个数是第一列的最大数，是最后一行的最小数；因此可以从左下角开始向上，右遍历；从右上角也是一样的，但是不能从另外两个角去遍历，因为数组的增大方向是从上到小，从左到右，不管先遍历哪个方向，当遇到大于或者小于target时，另一个方向去遍历时，下一个数依旧是大于或者小于target，因为在增大方向上下一个数必定是比前一个数大的，因此可以选择从左下角或者右上角遍历；这里拿左下角举例:如果左下角第一个数大于target那么就像上遍历，当上方向上的数小于target时，就想右遍历，若右方向的数大于target的话，再在此基础上向上遍历，直至找到target，若不存在target则直接返回False时间复杂度O(m+n)\nclass Solution:def searchMatrix(self, matrix: List[List[int]], target: int) -&gt; bool:    m = len(matrix) # 行    n = len(matrix[0]) # 列    # 行列为空的话直接返回false    if m == 0:        return False    if n == 0:        return False    # i 代表上下方向，j代表左右方向    i = m - 1     j = 0    while i&gt;=0 and j &lt; n:        if matrix[i][j] == target:            return True        # 小于target时向右移动，否则向上移动        elif matrix[i][j] &lt; target:            j = j + 1       else:            i = i - 1    return False\n\n275. H 指数 II\n二分法，关键是返回的区间理解好题目后才好下手,要理解h 指数是指他（她）的 （n 篇论文中）总共有 h 篇论文分别被引用了至少 h 次；题目的意思是返回满足条件的个数不是刚好满足条件的那个个论文引用次数，是所有大于这个条件的个数，比如[1,2]那么返回的就是1，引用次数大于2的为0，大于1的为1，又比如[0,1,3,5,6,8,9]返回的就是4，表示至少有四篇论文引用次数大于4其余的论文引用次数小于4。所以就可以用二分法的变式来解决该问题，时间复杂度为logn\nclass Solution:    def hIndex(self, citations: List[int]) -&gt; int:        low = 0        high = len(citations) - 1        while low &lt;= high:            mid = (low + high) // 2            # 判断当前论文引用次数是否大于当前引用次数论文的个数            if citations[mid] &gt;= len(citations) - mid:                high = mid - 1            else:                low =  mid + 1        # 当循环结束时low位置所指向的值的下一个就是第一个满足的值，因此用数组长度-当前low值就是所有满足的个数        return len(citations) - low\n\n逆转数组后一次遍历逆转数组后通过比较元素的索引和值来确定h值，逆转数组为了方便计算，i+1是因为要比较引用次数而索引下标是从0开始所以用i+1,然后从头开始遍历\nclass Solution:    def hIndex(self, citations: List[int]) -&gt; int:        # 反转        citations = citations[::-1]        length = len(citations)        index = 0        for i in range(length):            if citations[i] &gt;= i+1:                index = i+1        return index\n\n81. 搜索旋转排序数组 II\n直接遍历数组找target时间复杂度O(n),用for循环遍历数组看target是否在其中\n   class Solution:    def search(self, nums: List[int], target: int) -&gt; bool:        for i in range(len(nums)):            if nums[i] == target:                return True        return False   ```  2. 二分查找(官方解答)   由于数组中元素是可以有重复的，因此可能会出现`nums[left]=nums[mid]=nums[right]`的情况，对于这种情况，对区间的左边界减一，右边界加一来进行新的二分查找   ```python    class Solution:    def search(self,nums,target):        # 数组为空时，直接返回False        if len(nums)==0:            return False        # 数组进一个元素时，直接判断        if len(nums)==1:            return True if nums[0]==target else False        left,right=0,len(nums)-1        while left&lt;=right:            mid=(left+right)//2            if nums[mid]==target:                return True            # 划分数组,并判断            # 左边界的值=中间位置值=右边界的值时，构建新数组            if nums[left]==nums[mid]==nums[right]:                left+=1                right-=1            elif nums[left]&lt;=nums[mid]:                # 左边的数组是有序数组，且target存在于区间内，则移动右指针；否则，移动左指针                if nums[left]&lt;=target&lt;=nums[mid]:                    right=mid-1                else:                    left=mid+1            else:                # 右边的数组是有序数组，且target存在于区间内，则移动左指针；否则，移动否指针                if nums[mid]&lt;target&lt;=nums[len(nums)-1]:                    left=mid+1                else:                    right=mid-1        return False\n\n53. 最大子数组和\n贪心算法刚开始我是用的双循环去统计最大和，但是会超出时间复杂度，贪心算法的思想是:在每一步选择中都采用当前状态下最优的选择，而不考虑之后的结果,定义一个sum用来记录当前值与当前值和下一个数相加的大小，返回值大的那个给sum,这个sum就是前几项的和最大值，然后再将sum与待返回的数res做比较将值的那个赋值给res。\nclass Solution:     def maxSubArray(self, nums: List[int]) -&gt; int:         if not nums:             return NULL         res = sum = nums[0]         for i in range(1,len(nums)):             sum = max(nums[i],sum+nums[i]) # 更新前i个数中的最大和             res = max(sum,res) # 更新最大和         return res\n\n动态规划动态规划是一种解决复杂问题的算法思想，通常用于解决具有重叠子问题和最优子结构的问题。基本思想是将问题分解为更小的子问题，通过解决子问题来解决原始问题；将原数组在遍历时如果前一个数大于0则加到这个数上小于0则不变，最后遍历数组找出最大那个即可\n class Solution:    def maxSubArray(self, nums: List[int]) -&gt; int:        n = len(nums)        for i in range(1,n):            if nums[i-1] &gt; 0:                nums[i] += nums[i-1]        return max(nums)\n\n1480. 一维数组的动态和\n后一个加上前一个的累加，动态规划\nclass Solution: def runningSum(self, nums: List[int]) -&gt; List[int]:     for i in range(1,len(nums)):         nums[i] = nums[i] + nums[i-1]     return nums\n\n1991. 找到数组的中间位置\n动态规划用right_sum记录数组的和，然后再遍历数组，遍历一个减去一个，用left_sum记录遍历数组的和，当左等于右时，即为中间位置\nclass Solution: def findMiddleIndex(self, nums: List[int]) -&gt; int:     if len(nums) == 1:         return 0     left_sum,right_sum = 0,sum(nums)     for i in range(len(nums)):         right_sum -= nums[i]         if left_sum == right_sum:             return i         left_sum += nums[i]     return -1\n\n560. 和为 K 的子数组\n使用字典(看评论区大神解决思想)可以使用一个字典dic来记录数组的前缀和的个数,用变量sums记录前i个数的前缀和,存入字典当中,再定义变量res用来返回和为k的连续子数组个数，注意的是dic字典需要初始为&#123;0:1&#125;,代表连续子数组本身，代码如下:\nclass Solution: def subarraySum(self,nums:List[int],k:int) -&gt; int:     res = sums = 0     dic = &#123;0:1&#125;     for i in range(len(nums)):         sums += nums[i]         res += dic.get(sums - k,0) # sums - k 表示前i项和减去k的结果若存在再dic中，则说明前i项和中包含和为k的连续子数组，个数为sums - k 的值再dic中的次数         dic[sums] = dic.get(sums,0) + 1 # 记录每个前缀和出现的次数     return res\n\n739. 每日温度\n双循环暴力解(超出时间限制)使用两个for循环暴力求解，时间复杂度为O(n^2),核心思想就是在第二个for循环中去找下一个比当前值大的那个，使其下标相减即为相差的天数\nclass Solution: def dailyTemperatures(self, temperatures: List[int]) -&gt; List[int]:     answer = [0 for _ in range(len(temperatures))]     for i in range(len(temperatures)):         for j in range(i+1,len(temperatures)):             if temperatures[j] &gt; temperatures[i]:                 answer[i] = j - i                 break     return answer\n\n单调栈(评论区解答)创建个栈，用来判断温度的高低，栈中存放的是元素的下标，遍历原数组，若此时遍历元素大于栈中的元素，则栈中元素出栈，并记录，再用当前元素下标值减去出栈元素的值，然后放入结果数组ans的相应位置，若小于的情况下直接入栈。具体代码如下:\nclass Solution: def dailyTemperatures(self, temperatures: List[int]) -&gt; List[int]:     stack = [] # 存放元素的下标值     res = [0 for _ in range(len(temperatures))] # 初始化结果数组     for i in range(len(temperatures)):         while stack and temperatures[i] &gt; temperatures[stack[-1]]:             tmp = stack.pop()             res[tmp] = i - tmp         stack.append(i)     return res\n\n503. 下一个更大元素 II\n单调栈由于给定的是循环数组，那么在进行查找下个更大元素时，可以定义一个新的数组new_nums其大小为nums + nums形成一个循环数组，然后在同样的定义一个栈，用来判断元素的大小，栈中的存入的元素为数组的下标值，小于入栈，大于出栈，再把相应的值赋值给结果数组ans即可。注意的是最后只返回ans数组的len(nums)大小，而不是返回新数组的长度大小。代码如下:\nclass Solution: def nextGreaterElements(self, nums: List[int]) -&gt; List[int]:     new_nums = nums + nums # 新数组     stack = []     ans = [-1 for _ in range(len(new_nums))] # 初始化结果数组     for i in range(len(new_nums)):         while stack and new_nums[i] &gt; new_nums[stack[-1]]: # 判断元素的大小             tmp = stack.pop()             ans[tmp] = new_nums[i] # 存放的元素为上一个最大的元素         stack.append(i)     return ans[0:len(nums)] # 注意返回的长度\n\n使用取余优化操作(评论区大神)再判断数组值的大小的时候用i%len(nums)来判断，就可以减少结果数组占用的空间，防止操作过程中，数组溢出。\nclass Solution: def nextGreaterElements(self, nums: List[int]) -&gt; List[int]:     N = len(nums)     res = [-1] * N # 初始化结果数组     stack = []     for i in range(N * 2):         while stack and nums[stack[-1]] &lt; nums[i%N]: # 这个取余没想到，刚好满足数组长度的大小，不会造成溢出             res[stack.pop()] = nums[i%N]         stack.append(i%N)     return res\n\n42.接雨水\n左右向最高点遍历(评论区大神思想)总体思路是先找出数列中最高点的值和位置，若有多个相同最高点任取其一即可，在定义变量left_max记录左边的最高点，从左开始向数列最高点处遍历，若当前值大于left_max则更新left_max若小于这储水量res&#x3D;left_max - height[i]。遍历完左边走再将left_max初始化为0，再从右边向最高点遍历，最后返回res\nclass Solution: def trap(self, height: List[int]) -&gt; int:     left_max = 0 # 左最高点     max_high = max(height) # 最高点     max_index = height.index(max_high) # 最高点下标     res = 0     # 从左向最高点遍历     for i in range(max_index):         if height[i] &gt;= left_max:             left_max = height[i] # 更新左最高         else:             res += left_max - height[i]     left_max = 0     # 从右向最搞点遍历     for j in range(len(height)-1,max_index,-1):         if height[j] &gt;= left_max:             left_max = height[j]         else:             res += left_max - height[j]     return res\n\n单调栈使用单调栈求解，横向求储水高度，首先定义栈stack并先存入第一个元素,(注意栈中存放的数组的下标值),再从第一号元素开始遍历数组，若小于或者等于栈中的元素则入栈，保持递增栈的状态，若大于则说明存在空隙，出栈并使用变量mid记录当前的栈顶元素，再比较当前元素与栈顶元素的大小，取最小的减去mid位置的值，得到雨水的高度，再使用当前变量i减去栈顶元素的值，再减一得到当前雨水的宽度，要再减一是因为只要计算空袭位置的宽度，将宽度乘以高度即为空隙处的储水量；再使用while循环遍历这个过程，当循环结束后要把当前i值入栈；用于后续的计算；\nclass Solution: def trap(self, height: List[int]) -&gt; int:     stack = [0] # 存放下标     res = 0 # 记录储水量     for i in range(1,len(height)):         # 递增栈         while stack and height[i] &gt; height[stack[-1]]:             mid = stack.pop()             if stack:                 h = min(height[i],height[stack[-1]]) - height[mid] # 计算高                 w = i - stack[-1] -1 # 计算宽                 res += h * w         stack.append(i)     return res\n\n145. 二叉树的后序遍历\n递归在函数内部定义一个新的函数postoder用来实现递归调用，并使用res数组记录每一次递归调用的值，完成后序遍历\n# Definition for a binary tree node.# class TreeNode:#     def __init__(self, val=0, left=None, right=None):#         self.val = val#         self.left = left#         self.right = rightclass Solution:    def postorderTraversal(self, root: Optional[TreeNode]) -&gt; List[int]:        res = []        def postorder(root):            # 判空            if not root:                return list()            # 递归后序遍历，左右根            postorder(root.left)            postorder(root.right)            res.append(root.val)        postorder(root)        return res\n\n迭代(知道原理写不出代码，菜！)借助辅助栈实现，首先先遍历到二叉树的左边第一个结点，并把从途经结点存入栈中，再判断当前结点是否存在右孩子或者是否已经遍历过，若不存在右孩子，则出栈记录该节点，并把当前root设置为None；若不为None则无法完成继续操作，因为当前结点出栈记录后，接着应该继续出栈结点，来判断改该节点的右孩子和遍历情况；若该节点有右孩子，则入栈并且将root指向它的右孩子，再接着判断其右孩子的左右孩子情况；依次迭代\n# Definition for a binary tree node.# class TreeNode:#     def __init__(self, val=0, left=None, right=None):#         self.val = val#         self.left = left#         self.right = rightclass Solution:    def postorderTraversal(self, root: Optional[TreeNode]) -&gt; List[int]:        if not root:            return list()        prev = None # 记录前驱        stack = []        res = []        while root or stack:            # 找到二叉树最左边的第一个元素            while root:                stack.append(root)                root=root.left            # 更新root的值            root=stack.pop()            # 判断改结点是否存在右子树或者是否已经遍历过            if not root.right or root.right==prev:                res.append(root.val)                prev = root # 记录遍历过的元素                root = None # 方便继续弹出栈元素进行判断处理            # 当前结点存在右子树            else:                stack.append(root)                root=root.right        return res\n\nMorris遍历(大概率以后又忘了…)一种巧妙的方法在O(n)时间复杂度和O(1)空间复杂度情况下完成二叉树遍历，由J. H. Morris 在 1979 年的论文「Traversing Binary Trees Simply and Cheaply」中首次提出。其具体思想就是利用树的空闲指针，实现其空间开销的极限缩减(类似于线索二叉树?),其规则具体如下:\n\n新建临时结点，root;\n如果当前结点左子为空，则遍历当前结点的右子节点；\n如果当前结点的左子节点不为空，在当前节点的左子树中找到当前节点在中序遍历下的前驱结点；\n如果前驱结点的右子节点为空，将前驱结点的右子节点设置为当前节点,当前节点更新为当前节点的左子节点。\n如果前驱节点的右子节点为当前节点，将它的右子节点重新设置为空。倒序输出从当前节点的左子节点到该前驱节点这条路径上的所有结点，当前结点更新为当前结点的右子节点。\n\n\n重复步骤2和3，直到遍历结束\n\n # Definition for a binary tree node.# class TreeNode:#     def __init__(self, val=0, left=None, right=None):#         self.val = val#         self.left = left#         self.right = rightclass Solution:    def postorderTraversal(self, root: Optional[TreeNode]) -&gt; List[int]:        def addpath(node:TreeNode):            count = 0            while node:                count += 1                res.append(node.val)                node = node.right            i,j = len(res) - count, len(res) - 1 # i,j 默认初始值为res的第一个和最后一个值            # 逆置res             while i&lt;j:                res[i],res[j] = res[j],res[i]                i += 1                j -= 1        if not root:            return list()        res = list()        p1 = root        # 类似线索化二叉树，记录结点在中序遍历中的前驱位置        while p1:            p2=p1.left            if p2:                # 找到p2的最右节点                while p2.right and p2.right!=p1:                    p2 = p2.right                # 若p2不存在右孩子了，则将p2的右指针指向p1，p1指向其左孩子，继续循环                if not p2.right:                    p2.right = p1                    p1 = p1.left                    continue                # p2 存在其右孩子 而且p2的右指针指向当前p1节点，那么将p2的右孩子重新置为空，利用addpath函数逆序记录当前节点的左子节点到其前驱节点的所有结点                else:                    p2.right = None                    addpath(p1.left)            # 更新当前结点指向其右子节点，在其右子节点重复上述操作            p1 = p1.right                # 剩下结点为将root到最右结点的路径，逆序记录到res，即可完成后序遍历        addpath(root)        return res\n\n104. 二叉树的最大深度\n动态规划 （参考以前提交的c++代码)定义left_depth和right_depth分别记录左右子树的深度，然后调用自身函数，返回左右子树深度最大的一个再加一(加上跟根节点的高度)\n# Definition for a binary tree node.# class TreeNode:#     def __init__(self, val=0, left=None, right=None):#         self.val = val#         self.left = left#         self.right = rightclass Solution:    def maxDepth(self, root: Optional[TreeNode]) -&gt; int:        if not root:            return 0        left_depth = self.maxDepth(root.left)        right_depth = self.maxDepth(root.right)        # 加一是因为还有根节点的高度        return max(left_depth,right_depth)+1\n\n广度优先搜索(LC官方题解)使用辅助队列queue将每一层的元素存入队列，在使用ans记录层数，sz记录每一层的个数，用while循环来判断，判断每一个结点是否有左右孩子，若有则入队，每判断一个元素，sz减一，当一层的元素全部判断完后，层数ans加一.\n# Definition for a binary tree node.# class TreeNode:#     def __init__(self, val=0, left=None, right=None):#         self.val = val#         self.left = left#         self.right = rightclass Solution:    def maxDepth(self, root: Optional[TreeNode]) -&gt; int:        if not root:            return 0        queue=[]        # 将根结点先入队，否则层数会少一层        queue.append(root)        ans = 0        while len(queue):            sz = len(queue) # 记录每一层的元素个数            # 判断每一个元素是否有左右孩子，有则入队            while sz&gt;0:                Node = queue.pop(0)                if Node.left:                    queue.append(Node.left)                if Node.right:                    queue.append(Node.right)                sz -= 1            # 每一层元素判断完毕后，层数加一            ans += 1        return ans\n\n","categories":["力扣"],"tags":["python","刷题"]},{"title":"Spark3.2随笔","url":"/2023/11/06/Spark3-2%E9%9A%8F%E7%AC%94/","content":"Spark基础一、Spark简介定义：Apache Spark是用于大规模数据（large-scala data）处理的统一（unified）分析引擎。\nSpark 借鉴了 MapReduce 思想发展而来，保留了其分布式并行计算的优点并改进了其明显的缺陷。让中间数据存储在内存中提 高了运行速度、并提供丰富的操作数据的API提高了开发速度。\n\n统一分析引擎？\nSpark是一款分布式内存计算的统一分析引擎。 其特点就是对任意类型的数据进行自定义计算。\n Spark可以计算：结构化、半结构化、非结构化等各种类型的数据结构，同时也支持使用Python、Java、Scala、R以及SQL语言去开发应用 程序计算数据\n\nSpark VS Hadoop\nHadoop中的MR中每个map&#x2F;reduce task都是一个java进程方式运行，好处在于进程之间是互相独立的，每个task独享进程资源，没 有互相干扰，监控方便，但是问题在于task之间不方便共享数据，执行效率比较低.比如多个map task读取不同数据源文件需要将数据源加 载到每个map task中，造成重复加载和浪费内存。而基于线程的方式计算是为了数据共享和提高执行效率，Spark采用了线程的最小的执行 单位，但缺点是线程之间会有资源竞争.\n\nSpark的四大特点\n\n速度快\n易于使用\n通用性强\n多种运行方式\n\n\nSpark的架构角色\n\nMaster角色,管理整个集群的资源   –  类比于YARN的ResourceManager\n\nWork角色,管理单个服务器的资源  –  类比于YARN的NodeManager\n\nDriver角色,管理单个Spark任务在运行的时候的工作  –  类比于YARN的ApplicationMaster\n\nExecutor角色,单个任务运行的时候的一堆工作者，干活的 – 类比于YARN容器内运行的TASK\n从两个层面划分:\n\n资源管理层面:\n管理者:Spark是Master角色，YARN是ApplicationMaster\n工作者:Spark是Worker角色，YARN是NodeManager\n\n\n从任务执行层面:\n某任务管理者:Spark是Driver角色，YARN是ApplicationMaster\n某任务执行者:Spark是Executor角色，YARN是容器中运行的具体工作进程\n\n\n\n\n\n\n\n二、Spark环境搭建 -local2.1 基本原理本质：启动一个JVM Process进程(一个进程里面有多个线程)，执行任务Task\n\nLocal模式可以限制模拟Spark集群环境的线程数量, 即Local[N] 或 Local[*]\n\n其中N代表可以使用N个线程，每个线程拥有一个cpu core。如果不指定N，则默认是1个线程（该线程有1个core）。 通常Cpu有几个Core，就指定几个线程，最大化利用计算能力\n\n如果是local[*]，则代表 Run Spark locally with as many worker threads as  logical cores on your machine.按照Cpu最多的Cores设置线程数\n\n\nLocal 下的角色分布： \n资源管理： Master：Local\n进程本身 Worker：Local进程本身 \n任务执行： \nDriver：Local进程本身\n Executor：不存在，没有独立的Executor角色, 由Local进程(也就是Driver)内的线程提供计算能力\nPS: Driver也算一种特殊的Executor, 只不过多数时候, 我们将Executor当做纯Worker对待, 这样和Driver好区分(一类是管理 一类是工人)\n注意: Local模式只能运行一个Spark程序, 如果执行多个Spark程序, 那就是由多个相互独立的Local进程在执行\n2.2搭建2.2.1 安装条件\nSpark\n下载地址:https://dlcdn.apache.org/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz\n解压:tar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C /export/server/\n\nPython 推荐3.8\n\nJDK 1.8\n\nAnaconda on linux\n\n\n2.2.2 环境变量配置Spark由如下5个环境变量需要设置\n\nSPARK_HOME: 表示Spark安装路径在哪里 \nPYSPARK_PYTHON: 表示Spark想运行Python程序, 那么去哪里找python执行器 \nJAVA_HOME: 告知Spark Java在哪里 \nHADOOP_CONF_DIR: 告知Spark Hadoop的配置文件在哪里 \nHADOOP_HOME: 告知Spark  Hadoop安装在哪里\n\n这5个环境变量 都需要配置在: /etc/profile中\n在porfile文件输入以下配置指令:\n\nexport SPARK_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;sparkexport PYSPARK_PYTHON&#x3D;&#x2F;export&#x2F;server&#x2F;anaconda3&#x2F;envs&#x2F;pyspark&#x2F;bin&#x2F;python3.8export HADOOP_CONF_DIR&#x3D;$HADOOP_HOME&#x2F;etc&#x2F;hadoop\n\nPYSPARK_PYTHON和JAVA_HOME 需要同样配置在/root/.bashrc中 输入vim ~/.bashrc\n\nexport JAVA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;jdkexport PYSPARK_PYTHON&#x3D;&#x2F;export&#x2F;server&#x2F;anaconda3&#x2F;envs&#x2F;pyspark&#x2F;bin&#x2F;python3.8\n\n2.2.3 安装相关环境将所需安装包文件上传到&#x2F;export文件夹下\n\n安装anaconda\n在&#x2F;export 文件夹下，执行 sh ./Anaconda3-2021.05-Linux-x86_64.sh\n然后选择安装路径到/export/server/anaconda3目录下，执行安装操作\n\n\n是否执行初始化:yes\n\n\n输入source /export/server/anaconda3/bin/activate启动anaconda\n\n配置pyspark环境\n在anconda启动后输入 conda create -n pyspark python=3.8\n输入conda activate pyspark激活创建的虚拟环境\n\n\n安装spark\n在&#x2F;export文件夹下执行tar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C /export/server/\n为spark添加软链接 ln -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark\n\n\n2.2.4 启动测试先进入/export/server/spark目录下：\n\npyspark\n进入bin目录运行./pysparkpyspark 程序, 可以提供一个  交互式的 Python解释器环境, 在这里面可以写普通python代码, 以及spark代码\n\n\nWEB UI 4040\n4040端口是一个WEBUI端口, 可以在浏览器内打开输入:服务器ip:4040 即可打开:\n\n\nSpark-shell\n同样是一个解释器环境, 和bin/pyspark不同的是, 这个解释器环境 运行的不是python代码, 而是scala程序代码\n\nspark-submit (PI)\n作用: 提交指定的Spark代码到Spark环境中运行\n使用方法:\n# 语法bin/spark-submit [可选的一些选项] jar包或者python代码的路径 [代码的参数]# 示例bin/spark-submit/export/server/spark/examples/src/main/python/pi.py 10# 此案例 运行Spark官方所提供的示例代码 来计算圆周率值.  后面的10 是主函数接受的参数, 数字越高, 计算圆周率越准确.\n\n对比\n\n\n\n功能\nbin&#x2F;spark-submit\nbin&#x2F;pyspark\nbin&#x2F;spark-shell\n\n\n\n功能\n提交java\\scala\\python代码到spark中运行\n提供一个python\n\n\n\n解释器环境用来以python代码执行spark程序\n提供一个scala\n\n\n\n\n解释器环境用来以scala代码执行spark程序\n\n\n\n\n\n特点\n提交代码用\n解释器环境 写一行执行一行\n解释器环境 写一行执行一行\n\n\n使用场景\n正式场合, 正式提交spark程序运行\n测试\\学习\\写一行执行一行\\用来验证代码等\n测试\\学习\\写一行执行一行\\用来验证代码等\n\n\n\n\n三、环境搭建-Standalone3.1 Standalone架构Standalone模式是Spark自带的一种集群模式，不同于前面本地模式启动多个进程来模拟集群的环境，Standalone模式是真实地在多个机器之间搭建Spark集群的环境，完全可以利用该模式搭建多机器集群，用于实际的大数据处理。\nStandAlone 是完整的Spark运行环境,其中:\nMaster角色以Master进程存在, Worker角色以Worker进程存在 Driver和Executor运行于Worker进程内, 由Worker提供资源供给它们运行\n\n\nStandAlone集群在进程上主要有3类进程:\n\n主节点Master进程:\nMaster角色, 管理整个集群资源，并托管运行各个任务的Driver\n\n从节点Workers:\n Worker角色, 管理每个机器的资源，分配对应的资源来运行Executor(Task)； 每个从节点分配资源信息给Worker管理，资源信息包含内存Memory和CPU Cores核数\n\n历史服务器HistoryServer(可选):\nSpark Application运行完成以后，保存事件日志数据至HDFS，启动HistoryServer可以查看应用运行相关信息。\n\n\n\n\n\n\n3.2 Standalone 环境安装3.2.1 集群规划使用三台Linux虚拟机来组成集群环境, 非别是:\nnode1\\ node2\\ node3\nnode1运行: Spark的Master进程  和 1个Worker进程\nnode2运行: spark的1个worker进程\nnode3运行: spark的1个worker进程\n整个集群提供: 1个master进程 和 3个worker进程\n3.3.2 在所有机器配置环境变量\n复制anaconda到node2和node3\n运行：\nscp Anaconda3-2021.05-Linux-x86_64.sh node2:` pwd `/scp Anaconda3-2021.05-Linux-x86_64.sh node3:` pwd `/\n\n其余步骤和node1安装一致\n\n在其余两台虚拟机上配置pyspark虚拟环境\n\n将node1的&#x2F;etc&#x2F;profile 和 &#x2F;root&#x2F;.bashrc 新增的配置文件，复制到node2和node3中的相同文件中\n\n在node1将spark改成hadoop用户\nchown -R hadoop:hadoop spark*\n\n\n3.3.3 配置spark相关配置文件在node1进入到spark的配置文件目录中, cd $SPARK_HOME/conf\n配置workers文件\n# 改名, 去掉后面的.template后缀mv workers.template workers# 编辑worker文件vim workers# 将里面的localhost删除, 追加node1node2node3到workers文件内# 功能: 这个文件就是指示了  当前SparkStandAlone环境下, 有哪些worker\n\n配置spark-env.sh文件\n# 1. 改名mv spark-env.sh.template spark-env.sh# 2. 编辑spark-env.sh, 在底部追加如下内容## 设置JAVA安装目录JAVA_HOME=/export/server/jdk## HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoopYARN_CONF_DIR=/export/server/hadoop/etc/hadoop## 指定spark老大Master的IP和提交任务的通信端口# 告知Spark的master运行在哪个机器上export SPARK_MASTER_HOST=node1# 告知sparkmaster的通讯端口export SPARK_MASTER_PORT=7077# 告知spark master的 webui端口SPARK_MASTER_WEBUI_PORT=8080# worker cpu可用核数SPARK_WORKER_CORES=1# worker可用内存SPARK_WORKER_MEMORY=1g# worker的工作通讯地址SPARK_WORKER_PORT=7078# worker的 webui地址SPARK_WORKER_WEBUI_PORT=8081## 设置历史服务器# 配置的意思是  将spark程序运行的历史日志 存到hdfs的/sparklog文件夹中SPARK_HISTORY_OPTS=&quot;-Dspark.history.fs.logDirectory=hdfs://node1:8020/sparklog/ -Dspark.history.fs.cleaner.enabled=true&quot;\n\n在HDFS上创建程序运行历史记录存放的文件夹:\nhadoop fs -mkdir /sparkloghadoop fs -chmod 777 /sparklog\n\n配置spark-defaults.conf文件\n# 1. 改名mv spark-defaults.conf.template spark-defaults.conf# 2. 修改内容, 追加如下内容# vim中复制可能会导致全部注释掉，只需输入 set paste 再粘贴就行# 开启spark的日期记录功能spark.eventLog.enabled \ttrue# 设置spark日志记录的路径spark.eventLog.dir\t hdfs://node1:8020/sparklog/ # 设置spark日志是否启动压缩spark.eventLog.compress  true\n\n配置log4j.properties 文件 [可选配置]\n# 1. 改名mv log4j.properties.template log4j.properties\n\n将INFO改成WARN\n\n\n\n这个文件的修改不是必须的,  为什么修改为WARN. 因为Spark是个话痨\n会疯狂输出日志, 设置级别为WARN 只输出警告和错误日志, 不要输出一堆废话.\n\n3.3.4 将配置好的spark文件分发到其他服务器scp -r spark-3.2.0-bin-hadoop3.2 node2:/export/server/scp -r spark-3.2.0-bin-hadoop3.2 node3:/export/server/\n\n不要忘记, 在node2和node3上 给spark安装目录增加软链接\nln -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark\n检查：\n检查每台机器的:\nJAVA_HOME\nSPARK_HOME\nPYSPARK_PYTHON\n等等 环境变量是否正常指向正确的目录\n启动历史服务器:\nsbin/start-history-server.sh\n使用jps指令查看:\n\n\n使用 ps -ef|grep 57895 来查看jobhistoryserver的信息\n\n启动spark的Master和Worker进程\n# 启动全部master和workersbin/start-all.sh# 或者可以一个个启动:# 启动当前机器的mastersbin/start-master.sh# 启动当前机器的workersbin/start-worker.sh# 停止全部sbin/stop-all.sh# 停止当前机器的mastersbin/stop-master.sh# 停止当前机器的workersbin/stop-worker.sh\n\n查看Master的WEB UI\n默认端口master我们设置到了8080\n如果端口被占用, 会顺延到8081 …;8082… 8083… 直到申请到端口为止\n可以在日志中查看, 具体顺延到哪个端口上:\nService &#39;MasterUI&#39; could not bind on port 8080. Attempting port 8081.\n\n\n\n\n3.3 连接到StandAlone集群\nbin&#x2F;pyspark\n执行:\nbin/pyspark --master spark://node1:7077# 通过--master选项来连接 StandAlone集群# 不写 --master选项，默认是local模式运行\n\n\n\nbin&#x2F;spark-shell\nbin/spark-shell --master spark://node1:7077# 同样适用--master来连接到集群使用\n\nbin&#x2F;spark-submit(PI)\nbin/spark-submit --master spark://node1:7077 /export/server/spark/examples/src/main/python/pi.py 100# 同样使用--master来指定将任务提交到集群运行\n\n查看历史服务器WEB UI\n历史服务器的默认端口是: 18080\n我们启动在node1上, 可以在浏览器打开:\nnode1:18080来进入到历史服务器的WEB UI上.\n\n\n3.4 Spark应用架构Spark Application运行到集群上时，由两部分组成：Driver Program和Executors。\nDriver Program :\n\n相当于AppMaster，整个应用管理者，负责应用中所有Job的调度执行; \n运行JVM Process，运行程序的MAIN函数，必须创建SparkContext上下文对象；\n一个SparkApplication仅有一个\n\nExecutors:\n\n相当于一个线程池，运行JVM Process，其中有很多线程，每个线程运行一个Task任务，一个Task任务运行需要1 Core CPU，所以可以认为Executor中线程数就等于CPU Core核数；\n一个Spark Application可以有多个，可以设置个数和资源信息；\n\n\n\n用户程序从最开始的提交到最终的计算执行，需要经历以下几个阶段:\n\n用户程序创建 SparkContext 时，新创建的 SparkContext 实例会连接到 ClusterManager。 Cluster Manager 会根据用户 提交时设置的 CPU 和内存等信息为本次提交分配计算资源，启动 Executor.\nDriver会将用户程序划分为不同的执行阶段Stage，每个执行阶段Stage由一组完全相同Task组成，这些Task分别作用于待处理数据的不同分区。在阶段划分完成和Task创建后， Driver会向Executor发送 Task.\nExecutor在接收到Task后，会下载Task的运行时依赖，在准备好Task的执行环境后，会开始执行Task，并且将Task的运行状态 汇报给Driver.\nDriver会根据收到的Task的运行状态来处理不同的状态更新。 Task分为两种：一种是Shuffle Map Task，它实现数据的重新 洗牌，洗牌的结果保存到Executor 所在节点的文件系统中；另外一种是Result Task，它负责生成结果数据\nDriver 会不断地调用Task，将Task发送到Executor执行，在所有的Task 都正确执行或者超过执行次数的限制仍然没有执行成 功时停止\n\n3.5 Spark程序运行层次结构4040: 是一个运行的Application在运行的过程中临时绑定的端口,用以查看当前任务的状态.4040被占用会顺延到4041.4042等 （4040是一个临时端口,当前程序运行完成后, 4040就会被注销哦 ）。\n8080: 默认是StandAlone下, Master角色(进程)的WEB端口,用以查看当前Master(集群)的状态 。\n18080: 默认是历史服务器的端口, 由于每个程序运行完成后,4040端口就被注销了. 在以后想回看某个程序的运行状态就可以通过历史 服务器查看,历史服务器长期稳定运行,可供随时查看被记录的程序的运行过程。\n\n运行起来一个Spark Application\n/export/server/spark/bin/pyspark --master spark://node1:7077# 打开运行如下spark命令sc.textFile(&quot;hdfs://node1:8020/input/words.txt&quot;).flatMap(lambda line:line.split(&quot; &quot;)).map(lambda x:(x,1)).reduceByKey(lambda a,b:a+b).collect()\n\n打开其4040端口,并查看：\n\n\n在node1运行pyspark-shell，WEB UI监控页面地址：http://node1:4040\n可以发现在一个Spark Application中，包含多个Job，每个Job有多个Stage组成，每个Job执行按照DAG图进行的\n\n\n其中每个Stage中包含多个Task任务，每个Task以线程Thread方式执行，需要1Core CPU\n\nSpark Application程序运行时三个核心概念：Job、Stage、 Task\n\njob\n由多个 Task 的并行计算部分，一般 Spark 中的 action 操作（如 save、collect，后面进一步说明），会生成一个 Job。\n\nStage\njob 的组成单位,一个 Job 会切分成多个 Stage ，Stage 彼此之间相互依赖顺序执行，而每个 Stage 是多 个 Task 的集合，类似 map 和 reduce stage。\n\nTask\n被分配到各个 Executor 的单位工作内容，它是 Spark 中的最小执行单位，一般来说有多少个 Paritition （物理层面的概念，即分支可以理解为将数据划分成不同 部分并行处理），就会有多少个 Task，每个 Task 只会处 理单一分支上的数据。\n\n\n\n\n四、环境搭建-StandAlone HASpark Standalone集群是Master-Slaves架构的集群模式，和大部分的Master-Slaves结构集群一样，存在着Master 单点故障（SPOF）的问题。\n\n\n4.1高可用HA如何解决这个单点故障的问题，Spark提供了两种方案：\n\n基于文件系统的单点恢复(Single-Node Recovery with Local File System)–只能用于开发或测试环境。\n\n基于zookeeper的Standby Masters(Standby Masters with ZooKeeper)–可以用于生产环境。\nZooKeeper提供了一个Leader Election机制，利用这个机制可以保证虽然集群存在多个Master，但是只有一个是Active 的，其他的都是Standby。当Active的Master出现故障时，另外的一个Standby Master会被选举出来。由于集群的信息 ，包括Worker， Driver和Application的信息都已经持久化到文件系统，因此在切换的过程中只会影响新Job的提交，对 于正在进行的Job没有任何的影响。加入ZooKeeper的集群整体架构如下图所示：\n\n\n4.1.1 安装配置Zookeeper\n简介\nZooKeeper是一个分布式的，开放源码的分布式应用程序协调服务，是Hadoop和Hbase的重要组件。它是一个为分布式应用提供一致性服务的软件，提供的功能包括：配置维护、域名服务、分布式同步、组服务等。\n除了为Hadoop和HBase提供协调服务外，Zookeeper也被其它许多软件采用作为其分布式状态一致性的依赖，比如Kafka，又或者一些软件项目中，也经常能见到Zookeeper作为一致性协调服务存在。\nZookeeper不论是大数据领域亦或是其它服务器开发领域，涉及到分布式状态一致性的场景，总有它的身影存在。\n\n安装\nZookeeper是一款分布式的集群化软件，可以在多台服务器上部署，并协同组成分布式集群一起工作。\n\n在node1上操作下载zookeeper安装包并解压\n# 下载wget http://archive.apache.org/dist/zookeeper/zookeeper-3.6.3/apache-zookeeper-3.6.3-bin.tar.gz# 确保如下目录存在，不存在就创建mkdir -p /export/server# 解压tar -zxvf apache-zookeeper-3.6.3-bin.tar.gz -C /export/server\n\nnode 1上创建软链接\nln -s /export/server/apache-zookeeper-3.6.3-bin /export/server/zookeeper\n\n\n\nnode 1 上修改配置文件\n# zookeeper 提供了示例配置文件将其改名再修改即可cd /export/server/zookeeper/conf/mv zoo_sample.cfg zoo.cfgvim zoo.cfg# 修改如下配置dataDir=/export/server/zookeeper/dataclientPort=2181initLimit=5syncLimit=2server.1=node1:2888:3888server.2=node2:2888:3888server.3=node3:2888:3888\n\nnode 1 上配置myid (标识操作，防止机器间错乱)\n# 1. 创建Zookeeper的数据目录mkdir /export/server/zookeeper/data# 2. 创建文件，并填入1vim /export/server/zookeeper/data/myid# 在文件内填入1即可\n\nnode 1 操作将zookeeper复制到node2 和 node3\ncd /export/serverscp -r apache-zookeeper-3.6.3-bin node2:`pwd`/scp -r apache-zookeeper-3.6.3-bin node3:`pwd`/\n\nnode 2 操作\nln -s /export/server/apache-zookeeper-3.6.3-bin /export/server/zookeeper# 2. 修改myid文件vim /export/server/zookeeper/data/myid# 修改内容为2\n\nnode 3 操作\nln -s /export/server/apache-zookeeper-3.6.3-bin /export/server/zookeeper# 2. 修改myid文件vim /export/server/zookeeper/data/myid# 修改内容为3\n\n【在node1、node2、node3上分别执行】启动Zookeeper\n# 启动命令/export/server/zookeeper/bin/zkServer.sh start\t\t# 启动Zookeeper\n\n\n\n【在node1、node2、node3上分别执行】检查Zookeeper进程是否启动\njps# 结果中找到有：QuorumPeerMain 进程即可\n\n\n\n\n\n【node1上操作】验证Zookeeper\n/export/server/zookeeper/zkCli.sh# 进入到Zookeeper控制台中后，执行ls /# 如无报错即配置成功\n\n\n\n至此Zookeeper安装完成\n\n\n4.2 基于ZooKeeper实现HA4.2.1 Spark StandAlone HA环境搭建\n前提: 确保Zookeeper 和 HDFS 均已经启动\n\n先在spark-env.sh中, 删除: SPARK_MASTER_HOST=node1\n\n\n\n原因: 配置文件中固定master是谁, 那么就无法用到zk的动态切换master功能了.\n在spark-env.sh中, 增加:\nSPARK_DAEMON_JAVA_OPTS=&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=node1:2181,node2:2181,node3:2181 -Dspark.deploy.zookeeper.dir=/spark-ha&quot;# spark.deploy.recoveryMode 指定HA模式 基于Zookeeper实现# 指定Zookeeper的连接地址# 指定在Zookeeper中注册临时节点的路径\n\n将spark-env.sh 分发到每一台服务器上\nscp spark-env.sh node2:/export/server/spark/conf/scp spark-env.sh node3:/export/server/spark/conf/\n\n\n停止当前StandAlone集群\nsbin/stop-all.sh\n\n\n\n启动集群:\n# 在node1上 启动一个master 和全部workersbin/start-all.sh# 注意, 下面命令在node2上执行sbin/start-master.sh# 在node2上启动一个备用的master进程\n\n\n\n检查node2 的master进程绑定的端口 输入 netstat -anp | grep 57923\n\n\n发现在8083端口监听，浏览器输入 node2:8083 可以看到:\n\n\n或者 进入spark的logs文件夹查看master日志文件 也能发现其绑定在了8083端口上:\n\n\n4.2.2 测试主备切换进入node1 输入jps:\n\n\n准备指令kill -9 49432先不运行，先到node2上提交一个spark 任务到当前alive 的master上：\n# node2 在spark目录下操作bin/spark-submit --master spark://node1:7077 /export/server/spark/examples/src/main/python/pi.py 1000\n\n在活跃的master界面我们可以看到一个正在运行的spark程序:\n\n\n然后运行刚刚哪个kill指令将这个活跃的master kill掉 我们发现并不会影响程序的运行,依旧给出了正确结果:\n\n\n然后我们进入到 node2 上的备用master界面，发现其状态从STANDBY变成了ALIVE:\n\n\n现在回到node1重新启动spark-master：\nsbin/start-master.sh\n\n\n我们发现其状态变成了STANDBY\n\n结论 HA模式下, 主备切换 不会影响到正在运行的程序.\n最大的影响是 会让它中断大约30秒左右.\n\n五、Spark On YARN (重点)5.1 简介按照前面环境部署中所学习的, 如果我们想要一个稳定的生产Spark环境, 那么最优的选择就是构建:HA StandAlone集群。\n不过在企业中,都基本上会有Hadoop集群. 也就是会有YARN集群. 对于企业来说,在已有YARN集群的前提下在单独准备Spark StandAlone集群,对资源的利用就不高. 所以, 在企业中,多数场景下,会将Spark运行到YARN集群中。\nYARN本身是一个资源调度框架, 负责对运行在内部的计算框架进行资源调度管理.  作为典型的计算框架, Spark本身也是直接运行在YARN中, 并接受YARN的调度的. \n所以, 对于Spark On YARN, 无需部署Spark集群, 只要找一台服务器, 充当Spark的客户端, 即可提交任务到YARN集群 中运行.\n\nSpark On YARN 本质 ？\nMaster角色由YARN的ResourceManager担任. \nWorker角色由YARN的NodeManager担任. \nDriver角色运行在YARN容器内 或 提交任务的客户端进程中\n 真正干活的Executor运行在YARN提供的容器内\n\nSpark On YARN 需要 ?\n\nYARN 集群\nSpark 客户端工具， 比如spark-submit, 可以将Spark程序提交到YARN中\n需要被提交的代码程序 ，如spark&#x2F;examples&#x2F;src&#x2F;main&#x2F;python&#x2F;pi.py此示例程序,或我们后续自己开发的Spark任务\n\n\n\n\n\n\n\n5.2 环境搭建确保:\n\nHADOOP_CONF_DIR\nYARN_CONF_DIR\n\n在spark-env.sh以及环境变量中即可\n\n\n连接到YARN中：\n\nbin&#x2F;pyspark\nbin/pyspark --master yarn --deploy-mode client|cluster# --deploy-mode 选项是指定部署模式, 默认是 客户端模式# client就是客户端模式# cluster就是集群模式# --deploy-mode 仅可以用在YARN模式下\n\n\n注意: 交互式环境 pyspark  和 spark-shell  无法运行 cluster模式\n\n\n\n在yarn界面点击 applicationmaster会自动跳转到spark的master UI界面，这是由yarn的webproxysever提供的\n\n\nbin&#x2F;spark-shell\nbin/spark-submit --master yarn --deploy-mode client|cluster# 示例bin/spark-submit --master yarn /export/server/spark/examples/src/main/python/pi.py 100\n\n\n\n5.3 部署模式DeployModeSpark On YARN是有两种运行模式的,一种是Cluster模式一种是Client模式. 这两种模式的区别就是Driver运行的位置. \nCluster模式即:Driver运行在YARN容器内部, 和ApplicationMaster在同一个容器内 \n\n\nClient模式即:Driver运行在客户端进程中, 比如Driver运行在spark-submit程序的进程中\n\n\n\n两种模式的区别\n\n\n5.3.1 client 模式测试假设运行圆周率PI程序，采用client模式，命令如下:\n# 在hadoop用户下 cd到spark目录下 bin/spark-submit --master yarn --deploy-mode client --driver-memory 512m --executor-memory 512m --num-executors 3 --total-executor-cores 3 /export/server/spark/examples/src/main/python/pi.py 100\n\n打开spark 的历史服务器，我们会发现driver没有logs日志，因为，driver没有运行在yarn内部\n日志结果全部通过终端输出:\n\n\n\n\n5.3.2 cluster 模式测试同样的命令程序，采用cluster模式:\n# 在hadoop用户下 cd到spark目录下 bin/spark-submit --master yarn --deploy-mode cluster --driver-memory 512m --executor-memory 512m --num-executors 3 --total-executor-cores 3 /export/server/spark/examples/src/main/python/pi.py 100\n\n此时我们会发现，终端不会输出结果，此时我们在spark 的history服务器上点开driver 的日志我们看到其输出，或者在yarn管理界面点击logs同样能看到日志和结果输出\n\n\n\n\n5.4 两种模式详细流程在YARN Client模式下，Driver在任务提交的本地机器上运行，示意图如下:\n\n\n具体流程步骤如下： \n\nDriver在任务提交的本地机器上运行，Driver启动后会和ResourceManager通讯申请启动ApplicationMaster ;\n随后ResourceManager分配Container，在合适的NodeManager上启动ApplicationMaster，此时的 ApplicationMaster的功能相当于一个ExecutorLaucher，只负责向ResourceManager申请Executor内存；\nResourceManager接到ApplicationMaster的资源申请后会分配Container，然后ApplicationMaster在资源分 配指定的NodeManager上启动Executor进程； \nExecutor进程启动后会向Driver反向注册，Executor全部注册完成后Driver开始执行main函数；\n之后执行到Action算子时，触发一个Job，并根据宽依赖开始划分Stage，每个Stage生成对应的TaskSet，之后 将Task分发到各个Executor上执行\n\n在YARN Cluster模式下，Driver运行在NodeManager Contanier中，此时Driver与AppMaster合为一体，示意图如下:\n\n\n具体流程步骤如下：\n\n任务提交后会和ResourceManager通讯申请启动ApplicationMaster; \n随后ResourceManager分配Container，在合适的NodeManager上启动ApplicationMaster，此时的 ApplicationMaster就是Driver； \nDriver启动后向ResourceManager申请Executor内存，ResourceManager接到ApplicationMaster的资源申请 后会分配Container,然后在合适的NodeManager上启动Executor进程; \nExecutor进程启动后会向Driver反向注册; \nExecutor全部注册完成后Driver开始执行main函数，之后执行到Action算子时，触发一个job，并根据宽依赖开 始划分stage，每个stage生成对应的taskSet，之后将task分发到各个Executor上执行\n\n5.5 PySpark5.5.1 是什么PySpark我们前面使用过bin&#x2F;pyspark 程序, 要注意, 这个只是一个应用程序, 提供一个Python解释器执行环境来运行Spark任务 我们现在说的PySpark, 指的是Python的运行类库, 是可以在Python代码中:import pyspark PySpark 是Spark官方提供的一个Python类库, 内置了完全的Spark API, 可以通过PySpark类库来编写Spark应用程序, 并将其提交到Spark集群中运行.\nPySpark类库和标准Spark框架的简单对比:\n\n\n5.5.2 安装PySpark类库在 三台虚拟机上进入之前conda创建好的pyspark环境中，运行  pip install pyspark\n\n\n六、本机开发环境搭建6.1 本机PySpark环境配置\nHaddop DDL\n\n将hadoop-3.3.0文件夹复制到:D:\\study\\hadoop-3.3.0\n将文件夹内bin内的hadoop.dll复制到: C:\\Windows\\System32里面去\n.配置HADOOP_HOME环境变量指向 hadoop-3.3.0文件夹的路径\n\n配置这些的原因是: hadoop设计用于linux运行, 我们写spark的时候 在windows上开发 不可避免的会用到部分hadoop功能 为了避免在windows上报错, 我们给windows打补丁\n\nAnaconda和PySpark安装\n打开 anaconda prompt 执行以下操作:\n\n创建虚拟环境\nconda create --prefix=D:\\envs\\pyspark python=3.8\n--prefix 是一个选项，用于指定要创建的conda环境的路径\n切换到创建的虚拟环境\nconda activate D:\\envs\\pyspark\n\n在虚拟环境安装包\n\n\npip install pyhive pyspark jieba -i https://pypi.tuna.tsinghua.edu.cn/simple\n\n\n6.2 vscode配置Python解释器\n安装必要插件\nRemote-SSH\n然后点击remote-ssh的config设置，改成如下:\n\n\n然后连接虚拟机服务器，输入密码后可以进入到远程环境中，此时会自动找到虚拟机安装好的python环境\n\n\n6.2.1 PyCharm配置Python解释器\n加远程连接\n点击右下角添加新的解释器，然后选择ssh连接到虚拟机\n\n\n输入ip和用户连接服务器后，选择anaconda创建的虚拟环境解释器:\n\n\n6.3 应用入口:SparkContextSpark Application程序入口为：SparkContext，任何一个应用首先需要构建SparkContext对象，如下两步构建：\n\n创建SparkConf对象\n设置Spark Application基本信息，比如应用的名称AppName和应用运行Master\n\n第二步、基于SparkConf对象，创建SparkContext对象\n\n\nwordcount示例:\n# coding:utf-8from pyspark import SparkConf,  SparkContextif __name__ == &#x27;__main__&#x27;:    conf = SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;WordCountHelloWorld&quot;)    # 通过SparkConf对象构建SparkContext对象    sc = SparkContext(conf=conf)    # wordcount单词计数，读取hdfs上的文件，对其内部的单词统计出现次数    file_rdd = sc.textFile(&quot;hdfs://node1:8020/input/words.txt&quot;)    # 将单词进行切割，得到一个储存全部单词的集合对象    words_rdd = file_rdd.flatMap(lambda line:line.split(&quot; &quot;))    # 将单词转换为元组对象    words_with_one_rdd = words_rdd.map(lambda x:(x,1))    # 将元组的value 按照key来分组，对所有的value执行聚合操作    result_rdd = words_with_one_rdd.reduceByKey(lambda a,b : a + b)    # 通过collect方法收集rdd的数据打印输出结果    print(result_rdd.collect())\n\n如果本地文件和虚拟机上文件不同步的话可以尝试重新上传:\n\n\n注意:\n如果要在windows本地运行代码的话，需要配置pysaprk的环境变量\n\n\n\n\n运行原理:\n\n\n注意的是如果使用yarn模式的话，文件地址如果使用的是本地文件的话一定要保证每个机器都能访问到，要不然会报错。\nSpark Core一、RDD详解1.1 RDD是什么RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，一种容错的、可并行操作的数据结构，用于在集群应用程序中共享数据；是Spark中最基本的数据抽象，代表一个不可变、可分区、里面的元素可并行计算的集合。所有的运算以及操作都建立在 RDD 数据结构的基础之上。\n可以认为RDD是分布式的列表List或数组Array，抽象的数据结构，RDD是一个抽象类Abstract Class和泛型Generic Type\n\n\n1.2 RDD的五大特性前三个特征每个RDD都具备，后两个特征可选的\n\nRDD是有分区的\n\n\n用代码演示:\ncd 到spark目录下，输入bin/pyspark运行终端，输入如下代码：\n\n\n\n\n计算方法都会作用到每一个分区之上\n使用map将每个数扩大十倍，我们会发现所有分区的数据都会进行变化，并不只改变一个分区:\n\n\nRDD之间是有相互依赖关系（血缘关系)\n\n\nKV型RDD可以有分区器\n\n\n  默认分区器:Hash分区规则，可以手动设置一个分区器(rdd.paritionBy的方法来设置)\n  不是所有RDD都是Key-Value型。\n\nRDD分区数据的读取会尽量靠近数据所在的服务器\n因为这样可以走本地读取避免网络读取.本地读取性能 &gt;&gt; 网络读取\nspark会在确保并行计算能力的前提下，尽量确保本地读取，但这并不是100%的\n\n\n二、RDD编程入门2.1 程序执行入口 SparkContext对象Spark RDD 编程的程序入口对象是SparkContext对象(不论何种编程语言) 只有构建出SparkContext, 基于它才能执行后续的API调用和计算本质上, SparkContext对编程来说, 主要功能就是创建第一个RDD出来\n\n\n2.2 RDD的创建RDD的创建主要有两种方式:\n\n通过并行化集合创建(本地对象 转 分布式RDD)\n读取外部数据源(读取文件)\n\n并行化创建:\n代码演示:\n# coding:utf-8from pyspark import SparkConf, SparkContextif __name__ == &#x27;__main__&#x27;:    # 初始化执行环境，构建SparkContext对象    conf = SparkConf().setAppName(&quot;test&quot;).setMaster(&quot;local[*]&quot;)    sc = SparkContext(conf=conf)    # 演示通过并行化集合的方式去创建RDD，本地集合-&gt;分布式对象(RDD)    rdd = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9])    # parallelize没有给定分区数，那么其默认分区数是多少?根据cpu核心来决定    print(&quot;默认分区数:&quot;, rdd.getNumPartitions())    rdd = sc.parallelize([1, 2, 3], 3)    print(&quot;分区数:&quot;, rdd.getNumPartitions())    # collect方法是将RDD中每个分区的数据，都发送到Driver中，形成一个Python List对象    # collect:分布式 -&gt; 本地集合    print(&quot;rdd的内容是:&quot;, rdd.collect())\n\nAPI:\nrdd = sparkcontext.parallelize(参数1,参数2)# 参数1 集合对象，例如list# 参数2 分区数rdd.getNumPartitions() # 获取RDD分区数量，返回是int数字\n\n读取文件创建:\ncode：\n# coding:utf-8from pyspark import SparkConf, SparkContextif __name__ == &#x27;__main__&#x27;:    # 构建spark-context对象    conf = SparkConf().setAppName(&quot;test&quot;).setMaster(&quot;local[*]&quot;)    sc = SparkContext(conf=conf)    # 通过textfile Api 读取数据    file_rdd1 = sc.textFile(&quot;../data/input/words.txt&quot;)    print(&quot;默认读取分区:&quot;, file_rdd1.getNumPartitions())    print(&quot;file_rdd 内容:&quot;, file_rdd1.collect())    # 加最小分区的测试    file_rdd2 = sc.textFile(&quot;../data/input/words.txt&quot;, 3)    # 最小分区数是参考值，spark有自己的判断，给得太大spark并不会按照你给的来分配    file_rdd3 = sc.textFile(&quot;../data/input/words.txt&quot;, 100)    print(&quot;file_rdd2 分区数:&quot;, file_rdd2.getNumPartitions())    print(&quot;file_rdd3 分区数:&quot;, file_rdd3.getNumPartitions())\n\n\n\nAPI:\nsparkcontext.textfile(参数1，参数2)# 参数1，必填，文件路径 支持本地文件 支持hdfs 也支持一些s3协议等# 参数2，可选，表示最小分区数量\n\n\ntextFile  一般除了有很明确的指向性吗，一般情况下，不设置分区参数\n\n另外还有专门读取一堆小文件的API：\nsparkcontext.wholeTextfile(参数1，参数2)# 参数1，必填，文件路径 支持本地文件 支持hdfs 也支持一些s3协议等# 参数2，可选，表示最小分区数量\n\n\n这个api偏向于少量分区读取数据，文件的数据很小分区很多，导致shuffle的几率更高，所以尽量少分区读取数据\n\n2.3 RDD算子算子:分布式集合对象的api称之为算子\n方法\\函数:本地对象的api，叫做方法\\函数\nRDD的算子分为两类:\n\nTransformation:转换算子\nAction:动作算子\n\n\nTransformation 算子\n\nRDD算子，返回值仍然是一个RDD，称之为转换算子；这类算子是lazy 懒加载的，如果没有action 算子，Transformation算子是不工作的\n\nAction 算子\n\n返回值不是rdd的就是action算子\n\n对于这两类算子来说,Transformation算子相当于在构建执行计划，action是一个指令让这个执行计划开始工作.如果没有action,Transformation算子之间的迭代关系就是一个没有通电的流水线只有action到来,这个数据处理的流水线才开始工作.\n\n2.4 常用Transformation算子\nmap算子\nmap算子 是将RDD的数据一条条处理，返回新的RDD\n语法:\n\nrdd.map(func)\n#接收参数传入传入类型不限，返回一个返回值，返回值类型不限\n\n\nflatMap算子\n对rdd执行map操作，然后进行接触嵌套操作\n解除嵌套:\n# 嵌套的listlst = [[1,2,3],[4,5,6],[7,8,9]]# 解除了嵌套lst = [1,2,3,4,5,6,7,8,9]\n\n演示代码:\n# coding:utf-8# 演示flatmap算子from pyspark import SparkConf, SparkContextif __name__ == &#x27;__main__&#x27;:    conf = SparkConf().setAppName(&quot;test&quot;).setMaster(&quot;local[*]&quot;)    sc = SparkContext(conf=conf)    rdd = sc.parallelize([&quot;a b c&quot;, &quot;a c e &quot;, &quot;e c a&quot;])    # 按照空格切分数据后，接触嵌套    print(rdd.flatMap(lambda x: x.split(&quot; &quot;)).collect())\n\n\n\n\n\nreduceByKey算子\n针对KV型RDD，自动按照key分组，然后根据提供的聚合逻辑，完成组内数据的聚合操作\n用法：\nrdd.reduceByKey(func)# func:(V,V) -&gt; V# 接收两个传入参数(类型要一致),返回一个返回值，类型和传入要求一致\n\n代码:\n# coding:utf-8from pyspark import SparkConf, SparkContextif __name__ == &#x27;__main__&#x27;:    conf = SparkConf().setAppName(&quot;test&quot;).setMaster(&quot;local[*]&quot;)    sc = SparkContext(conf=conf)    rdd = sc.parallelize([(&#x27;a&#x27;, 1), (&#x27;a&#x27;, 1), (&#x27;b&#x27;, 1), (&#x27;b&#x27;, 1), (&#x27;b&#x27;, 1)])    res = rdd.reduceByKey(lambda a, b: a + b)    print(res.collect())\n\n\n\n\nreduceByKey 中接收的函数，只负责聚合，不理会分组，分组是自动by key来分组的\n\n\nmapValues 算子\n针对二元元组RDD，对其内部的二元元组Value执行map操作\n语法:\nrdd.mapValues(func)# func:(V) -&gt; U  -&gt;U表示返回值# 传入的参数是二元元组的value值，只value进行处理\n\n代码:\n# coding:utf-8from pyspark import SparkConf,SparkContextif __name__ == &#x27;__main__&#x27;:    conf = SparkConf().setAppName(&quot;test&quot;).setMaster(&quot;local[*]&quot;)    sc = SparkContext(conf=conf)    rdd = sc.parallelize([(&#x27;a&#x27;,1),(&#x27;a&#x27;,11),(&#x27;a&#x27;,6),(&#x27;b&#x27;,3),(&#x27;b&#x27;,5)])    # 将二元元组的所有value都乘以10进行处理    print(rdd.mapValues(lambda x:x*10).collect())\n\n\n\n\n\ngroupBy 算子\n将rdd的数据进行分组\n语法:\nrdd.groupBy(func)# 传入一个函数，返回一个返回值，类型无所谓# 这个函数是拿到你的返回值后，将所有相同返回值的放入一个组中# 分组完成后，每一个组是一个二元组，key就是返回值，所有同组的数据放入一个迭代器对象中作为value\n\n代码:\n# coding:utf-8from pyspark import SparkConf, SparkContextif __name__ == &#x27;__main__&#x27;:    conf = SparkConf().setAppName(&quot;test&quot;).setMaster(&quot;local[*]&quot;)    sc = SparkContext(conf=conf)    rdd = sc.parallelize([(&#x27;a&#x27;, 1), (&#x27;a&#x27;, 1), (&#x27;b&#x27;, 1), (&#x27;b&#x27;, 2), (&#x27;b&#x27;, 3)])    # 通过groupby对数据进行排序    res = rdd.groupBy(lambda x: x[0])    # lambda函数意思是将元组的value强制转换成list对象 下面两种方法都可    print(res.mapValues(lambda x:list(x)).collect())    # print(res.map(lambda x: (x[0], list(x[1]))).collect())\n\n\n\nFilter 算子\n过滤想要的数据进行保留\n语法:\nrdd.filter(func)# func: (T) -&gt; bool 传入一个参数随意类型，返回值必须是t or f# t被保留，f被丢弃\n\n代码:\n# coding:utf-8from pyspark import SparkConf, SparkContextif __name__ == &#x27;__main__&#x27;:    conf = SparkConf().setAppName(&quot;test&quot;).setMaster(&quot;local[*]&quot;)    sc = SparkContext(conf=conf)    rdd = sc.parallelize([1, 2, 3, 4, 5])    # 保留奇数    print(rdd.filter(lambda x: x % 2 == 1).collect())\n\n\n\ndistinct 算子\n对RDD数据进行去重，返回新的RDD\n语法:\nrdd.dinstinct(参数1)# 参数1，去重分区数量，一般不用传\n\n代码:\n# coding:utf-8from pyspark import SparkConf, SparkContextif __name__ == &#x27;__main__&#x27;:    conf = SparkConf().setAppName(&quot;test&quot;).setMaster(&quot;local[*]&quot;)    sc = SparkContext(conf=conf)    rdd = sc.parallelize([1, 2, 2, 3, 4, 5, 5, 6, 7, 6, 6])    print(rdd.distinct().collect())\n\n\n\njoin 算子\n对两个RDD执行join操作(可实现SQL的内\\外连接)\njoin算子只能用于二元元组\n语法:\nrdd.join(other_rdd)rdd.leftOuterJoin(other_rdd) # 左外rdd.rightOuterJoin(oter_rdd) # 右外\n\n代码:\n# coding:utf-8from pyspark import SparkConf, SparkContextif __name__ == &#x27;__main__&#x27;:    conf = SparkConf().setAppName(&quot;test&quot;).setMaster(&quot;local[*]&quot;)    sc = SparkContext(conf=conf)    # 部门id和员工姓名    x = sc.parallelize([(1001, &quot;a&quot;), (1002, &quot;bb&quot;), (1003, &quot;ccc&quot;), (1004, &quot;dddd&quot;)])    # 部门id和部门名称    y = sc.parallelize([(1001, &quot;sales&quot;), (1002, &quot;tech&quot;)])    # join是内连接    # 对于join算子来说关联条件，按照二元元组的key来进行关联    print(x.join(y).collect())    # leftOuterjoin是左外连接 同理还有右外连接    print(x.leftOuterJoin(y).collect())\n\nunion 算子\n2个rdd合并成1个rdd返回\n用法:\nrdd.union(other_rdd)\n注意:\n\n\n只合并，不会去重\n不同类型的rdd依旧可以混合\n\n\n代码:\n# coding:utf-8from pyspark import SparkConf, SparkContextif __name__ == &#x27;__main__&#x27;:    conf = SparkConf().setAppName(&quot;test&quot;).setMaster(&quot;local[*]&quot;)    sc = SparkContext(conf=conf)    rdd1 = sc.parallelize([1, 1, 2, 3, 4])    rdd2 = sc.parallelize([5, 5, 4, 3, 6, 8])    union_rdd = rdd1.union(rdd2)    print(union_rdd.collect())\n\n\n\nintersection 算子\n求2个rdd的交集，返回一个新的rdd\n用法:\nrdd.intersection(other_rdd)\n代码:\n# coding:utf-8from pyspark import SparkConf, SparkContextif __name__ == &#x27;__main__&#x27;:    conf = SparkConf().setAppName(&quot;test&quot;).setMaster(&quot;local[*]&quot;)    sc = SparkContext(conf=conf)    rdd1 = sc.parallelize([(&#x27;a&#x27;, 1), (&#x27;b&#x27;, 1)])    rdd2 = sc.parallelize([(&#x27;a&#x27;, 1), (&#x27;c&#x27;, 1)])    rdd3 = rdd1.intersection(rdd2)    print(rdd3.collect())   \n\n\n\nglom 算子\n将RDD的数据，加上嵌套，这个嵌套按照分区来进行\n比如rdd数据[1,2,3,4,5]有两个分区，那么，被glom后数据变成[[1,2,3],[4,5]]\n使用方法:\nrdd.glom()\n与GetNumPartitions不同的是，GetNumPartions只能看到分区数目，而glom算子能看到每个分区的数据分布；\n代码：\n# coding:utf-8from pyspark import SparkConf, SparkContextif __name__ == &#x27;__main__&#x27;:    conf = SparkConf().setAppName(&quot;test&quot;).setMaster(&quot;local[*]&quot;)    sc = SparkContext(conf=conf)    rdd = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9], 3)    print(rdd.glom().collect())\n\n\n\ngroupBykey 算子\n针对KV型RDD，自动按照key分组\n用法:\nrdd.groupBykey()自动按照key分组\n代码:\n# coding:utf-8from pyspark import SparkConf, SparkContextif __name__ == &#x27;__main__&#x27;:    conf = SparkConf().setAppName(&quot;test&quot;).setMaster(&quot;local[*]&quot;)    sc = SparkContext(conf=conf)    rdd = sc.parallelize([(&#x27;a&#x27;, 1), (&#x27;b&#x27;, 2), (&#x27;a&#x27;, 1), (&#x27;b&#x27;, 2), (&#x27;a&#x27;, 1), (&#x27;c&#x27;, 2)])    group_rdd = rdd.groupByKey()    print(group_rdd)\n\n\n\nsortBy算子\n对RDD数据进行排序，基于指定的排序依据\n语法:\nrdd.sortBy(func,ascending=False,numPartitions=1)# func:(T) -&gt; U:告知按照rdd中的哪个数据进行排序，例如lambda x:x[1]表示按照rdd中的第二列元素进行排序# ascdeing True 升序 False 降序# numPartions:用多少分区排序\n\n注意:若要全局有序，排序分区数请设置为1\n代码:\n# coding:utf-8from pyspark import SparkConf, SparkContextif __name__ == &#x27;__main__&#x27;:    conf = SparkConf().setAppName(&quot;test&quot;).setMaster(&quot;local[*]&quot;)    sc = SparkContext(conf=conf)    rdd = sc.parallelize([(&#x27;a&#x27;, 1), (&#x27;a&#x27;, 2), (&#x27;a&#x27;, 1), (&#x27;a&#x27;, 3), (&#x27;d&#x27;, 2), (&#x27;c&#x27;, 3), (&#x27;f&#x27;, 5)])    # 按value排序    print(rdd.sortBy(lambda x: x[1], ascending=True, numPartitions=2).collect())    # 按key排序    print(rdd.sortBy(lambda x: x[0], ascending=False, numPartitions=1).collect())\n\n\n\nsortByKey 算子\n针对KV型RDD,按照key进行排序\n语法:\nsortByKey(ascending=True,numpartitons=None,keyfuc=&lt;function RDD.lambda &gt;)\nkeyfunc：再排序前对key进行处理，语法是(K) -&gt; U 一个参数传入，返回一个值\n代码:\n# coding:utf-8from pyspark import SparkConf, SparkContextif __name__ == &#x27;__main__&#x27;:    conf = SparkConf().setAppName(&quot;test&quot;).setMaster(&quot;local[*]&quot;)    sc = SparkContext(conf=conf)    rdd = sc.parallelize([(&#x27;a&#x27;, 1), (&#x27;E&#x27;, 1), (&#x27;d&#x27;, 1), (&#x27;V&#x27;, 1), (&#x27;f&#x27;, 1), (&#x27;p&#x27;, 1),                          (&#x27;n&#x27;, 1), (&#x27;m&#x27;, 1), (&#x27;L&#x27;, 1), (&#x27;r&#x27;, 1), (&#x27;h&#x27;, 1), (&#x27;j&#x27;, 1)], 3)    # 这里key是str但是通过强转之后可以调用lower()方法，将其转换成小写，防止大小写影响结果    print(rdd.sortByKey(ascending=True, numPartitions=1, keyfunc=lambda key: str(key).lower()).collect())\n\n\n\n\n案例\n\n需求:读取data文件夹中的order.txt文件，提取北京的数据，组合北京和商品类别进行输出，同时对结果集进行去重，得到北京售卖的商品类别信息\n代码:\n# coding:utf-8from pyspark import SparkConf, SparkContextimport jsonif __name__ == &#x27;__main__&#x27;:    conf = SparkConf().setAppName(&quot;case&quot;).setMaster(&quot;local[*]&quot;)    sc = SparkContext(conf=conf)    rdd = sc.textFile(&quot;../data/input/order.text&quot;)    # 提取每条json数据    json_str_rdd = rdd.flatMap(lambda x: x.split(&quot;|&quot;))    # 将json数据转换成python字典对象,json.loads()的作用是将 JSON 格式的字符串解析为 Python 数据结构，例如字典或列表    json_dict_rdd = json_str_rdd.map(lambda x: json.loads(x))    # print(json_dict_rdd.collect())    # 过滤数据，只保留北京    beijing_rdd = json_dict_rdd.filter(lambda t: t[&#x27;areaName&#x27;] == &#x27;北京&#x27;)    # print(beijing_rdd.collect())    # 组合北京和商品类型    res_rdd = beijing_rdd.map(lambda s: s[&#x27;areaName&#x27;] + &#x27;_&#x27; + s[&quot;category&quot;])    print(res_rdd.collect())\n\n\n\n将案例部署到集群运行:\n这里注意使用命令:hadoop fs -chmod 777 /user将hdfs文件文件夹权限修改，运行完后可以通过hadoop fs -chmod 755 /user修改回来，这里我提交到yarn运行时一直报错：root is not a leaf queue将yarn的配置文件yarn-site.xml中调度器的fair改成capacity再重启resourcemanager即可\n\n\n\n\n","categories":["BigData"],"tags":["Spark"]},{"title":"RCNN","url":"/2024/03/15/RCNN/","content":"RCNN 两阶段目标检测的开山之作【论文特点】:\n\n速度:本文则(采用Selective Search方法)预先提取一系列较可能是物体的候选区域，之后仅在这些候选区域上(采用CNN)提取特征，进行判断。在PASCAL VOC 2012数据集上的性能比之前最好的方法提高了30%.\n两阶段训练策略：文章提出了一种两阶段训练策略，首先在大型辅助数据集（如ImageNet）上进行有监督的预训练，然后在特定领域数据集（如PASCAL VOC）上进行微调，以适应目标检测任务。\n\nRCNN算法基本流程RCNN算法分为4个步骤:\n\n候选区域生成： 一张图像生成1K~2K个候选区域 （采用Selective Search 方法）\n特征提取： 对每个候选区域，使用深度卷积网络提取特征 （CNN）\n类别判断： 特征送入每一类的SVM 分类器，判别是否属于该类\n位置精修： 通过非极大值抑制、边界框回归、微调等方式精细修正候选框位置\n\n\n\nRCNN主要思想和方法Selective Search 主要思想:\n\n使用一种过分割手段，将图像分割成小区域 (1k~2k 个)\n查看现有小区域，按照合并规则合并可能性最高的相邻两个区域。重复直到整张图像合并成一个区域位置\n输出所有曾经存在过的区域，所谓候选区域\n\n其中合并规则如下： 优先合并以下四种区域：\n\n颜色（颜色直方图）相近的\n纹理（梯度直方图）相近的\n合并后总面积小的： 保证合并操作的尺度较为均匀，避免一个大区域陆续“吃掉”其他小区域\n合并后，总面积在其BBOX中所占比例大的： 保证合并后形状规则。\n\n有监督预训练与无监督预训练:\n(1)无监督预训练(Unsupervised pre-training)\n预训练阶段的样本不需要人工标注数据，所以就叫做无监督预训练。\n(2)有监督预训练(Supervised pre-training)\n所谓的有监督预训练也可以把它称之为迁移学习。比如你已经有一大堆标注好的人脸年龄分类的图片数据，训练了一个CNN，用于人脸的年龄识别。然后当你遇到新的项目任务时：人脸性别识别，那么这个时候你可以利用已经训练好的年龄识别CNN模型，去掉最后一层，然后其它的网络层参数就直接复制过来，继续进行训练，让它输出性别。这就是所谓的迁移学习，说的简单一点就是把一个任务训练好的参数，拿到另外一个任务，作为神经网络的初始参数值,这样相比于你直接采用随机初始化的方法，精度可以有很大的提高。\n重叠度（IOU）:\nIoU（Intersection over Union）是一种评估目标检测和实例分割算法性能的常用指标.它是预测边界框（预测的区域）与真实边界框（真实目标的区域）之间的重叠程度的度量。物体检测需要定位出物体的bounding box，就像下面的图片一样，我们不仅要定位出车辆的bounding box 我们还要识别出bounding box 里面的物体就是车辆。\n\n\n因此出现了IOU,它定义了两个bounding box的重叠度，如下图所示：\n\n\n就是矩形框A、B的重叠面积占A、B并集的面积比例。\n非极大值抑制（NMS）：\n非极大值抑制（NMS）顾名思义就是抑制不是极大值的元素，搜索局部的极大值。这个局部代表的是一个邻域，邻域有两个参数可变，一是邻域的维数，二是邻域的大小.NMS的主要作用是去除多余的检测框，只保留最佳的检测结果，从而提高检测的准确性和效率。\nNMS的实现通常遵循以下步骤：\n\n设置IoU阈值：确定一个IoU阈值，用于判断检测框之间的重叠程度。\n得分排序：根据检测框的置信度得分进行降序排序。\n迭代抑制：从得分最高的检测框开始，对于每个检测框，如果它与其他检测框的IoU超过阈值，则抑制那些得分较低的检测框。\n选择最佳检测框：在每个类别中，选择剩余的、未被抑制的检测框作为最终的检测结果。\n\nVOC物体检测任务:\n相当于一个竞赛，里面包含了20个物体类别：PASCAL VOC2011 Example Images 还有一个背景，总共就相当于21个类别，因此一会设计fine-tuning CNN的时候，我们softmax分类输出层为21个神经元。\nCNN特征提取实现\n算法实现\n选用Alexnet的5个卷积层和2个全连接层\n使用有监督预训练\nfine-tuning训练\n\n\n\nSVM训练、测试:这是一个二分类问题，我么假设我们要检测车辆。我们知道只有当bounding box把整量车都包含在内，那才叫正样本；如果bounding box 没有包含到车辆，那么我们就可以把它当做负样本。但问题是当我们的检测窗口只有部分包含物体，那该怎么定义正负样本呢？作者测试了IOU阈值各种方案数值0,0.1,0.2,0.3,0.4,0.5。最后通过训练发现，如果选择IOU阈值为0.3效果最好（选择为0精度下降了4个百分点，选择0.5精度下降了5个百分点）,即当重叠度小于0.3的时候，我们就把它标注为负样本。一旦CNN f7层特征被提取出来，那么我们将为每个物体类训练一个svm分类器。\n位置精修： 目标检测问题的衡量标准是重叠面积：许多看似准确的检测结果，往往因为候选框不够准确，重叠面积很小。故需要一个位置精修步骤。 回归器：对每一类目标，使用一个线性脊回归器进行精修。正则项λ&#x3D;10000。 输入为深度网络pool5层的4096维特征，输出为xy方向的缩放和平移。 训练样本：判定为本类的候选框中和真值重叠面积大于0.6的候选框。\n","categories":["论文"],"tags":["cv","dl"]},{"title":"ETD-NET笔记","url":"/2024/05/22/ETD-NET%E7%AC%94%E8%AE%B0/","content":"ETD-NETETDNet: Efficient Transformer-Based Detection Network for Surface Defect Detection\nIEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, VOL. 72, 2023\n\n\n设计三种新颖的设计\n\n引入改进的(M-LVT)提供了比基于cnn的模型更强的全局表示能力，同时保持了轻量级的设计。\n设计了一个信道调制特征金字塔网络(CM-FPN)，该网络具有FPN结构和全局上下文块(GC-Block)。\n提出了一个面向任务的解耦(TOD)头。它使用解耦策略来减少分类任务和回归任务之间的冲突，以及一个面向任务的策略来学习面向相应任务的特征。在TOD头部，提出了局部特征表示(LFR)模块，为分类任务有效地提取对象感知的局部特征。为此专门设计了一个全局特征表示(GFR)模块\n\n相关工作通用目标检测\n\nRCNN\n参数和计算量多，不切实际，在现实环境中受限\n\nOne-Stage 目标检测\n\nFCOS\n无锚点探测器，它去除广泛使用的锚点，直接从点回归。\n\n\n\n\n缺陷检测\n与传统方法相比，基于dl的缺陷检测具有更好的泛化性能和更快的检测速度。\n\nFPNs + DCNs 引入Faster RCNN中用于检测小而复杂的钢缺陷\n利用信道关注和自适应空间特征融合提高了retanet的检测性能\n采用了跳跃层连接模块(SCM)和金字塔特征融合模块(PFM)来融合多尺度特征\n利用信道关注和双向特征融合对FCOS网络进行了改进\n在yolov5中加入了transformer\n在编码器-解码器网络(EDNet)中引入了交叉注意转换器(CAT)用于表面缺陷检测\n\n注意力机制\n\nSENet\nSENet提出了一种通道注意机制，在图像分类任务中取得了良好的性能.\tSENet首先通过全局平均池化(GAP)将每个通道转换为一个值，然后使用两个完全连接的层来获得通道间的关系\n\n空间减少注意力(SRA)\n\n\n方法\nVIT\nViT块由两部分组成:自关注和前馈网络(FFN)。FFN是ViT块中的另一个模块，可以进一步捕获潜在的特征模式\n\n缺点:当ViT直接应用于需要高分辨率输入的缺陷检测场景时，其自关注模块的计算负担是压倒性的，无法承受的。\n\n\n\n总体架构\nbackbone \n\nM-LVT\n轻量级VIT模型；LVT它采用标准的四阶段分层设计；两个模块:包括CSA模块(卷积自注意力模块)和RASA模块(多尺度空洞卷积模块),CSA模块嵌入在第一阶段，RASA嵌入在其余阶段。CSA模块在大小为3 × 3的核内的卷积中引入自我注意力，增强卷积层的特征提取能力。STEM+CSA代表了两个模块的组合：STEM模块和CSA（卷积自注意力）模块。STEM模块用于在浅层对特征图进行下采样并捕获更多语义信息。RASA模块通过递归自关注(递归深度为2)来捕获多尺度全局特征，通过将STEM模块与CSA模块结合，ETDNet在特征提取的初始阶段能够更有效地处理输入数据，提高模型整体的检测性能。\n缺陷:当LVT应用于缺陷检测任务时1.在LVT中堆叠过多的层会导致检测器的效率降低。2.LVT的主干(stride&#x3D;4的卷积层)可能会丢失一些语义细节,3.由于缺陷检测和定位需要比图像分类任务更大的接受域，RASA中的扩张率不足以用于缺陷检测任务\n改进:\n\n将四个阶段的层数重新排列为[1,1,2,2]，而不是原始LVT中的[2,2,2]\n设计新的阀杆，利用一个卷积层(3 × 3, stride&#x3D;2)对feature map进行下采样，将通道维数从3扩展到32。使用两个并行分支(包括卷积分支和平均池化分支)进一步对特征映射进行下采样。将两个分支的输出连接并馈送到1 × 1卷积层中以进一步细化。由于其在浅层的线索保留能力，所提出的词干可以捕获更多的语义信息。\n原始的RASA使用三种扩张速率(1,3,5)的扩张卷积。引入了一个额外的扩张速率7来形成一个更大的接受野。\n\n\n\n\nNeck\n\nCM-FPN\n信道FPN ，GC-Block+FPN\nGC-Block首先设计了一个独立于查询的简化非局部网络,计算一个特征点的自关注来表示整个特征图的关注。然后，GCNet采用挤压和激励(SE)块通过两个FC层对信道关系进行建模。\n\n\n\nHead\n训练的卷积层是内容不可知的，捕获的局部特征适用于分类任务，而不适用于回归任务。原因是分类任务只需要响应对象的特定特征。相比之下，回归任务需要考虑全局特征来定位各种形状和规模缺陷的边界。\n\nTOD头\nLFR模块和GFR模块来解决两个任务。LFR通过卷积和通道关注模块学习对象感知的局部特征以响应特定对象。GFR采用内容自适应变压器模块来提高GFR，从而更好地定位缺陷。\n\nLFR模块\n提出了一个由标准卷积层和高效压缩激励(ESE)块组成的LFR模块。ESE块首先通过GAP将每个通道转换为一个值。然后，它采用FC层，然后Swish激活来获得信道缩放因子。最后，它重新缩放输入特征映射的每个通道，以建模通道关系。ESE通过重新校准每个通道的权重，从通道维度增强对象的特定特征。\n\nGFR模块\n由三个部分组成:信道自注意(CA)、深度卷积(DW)和FFN。与训练后权重固定的标准卷积层不同，卷积层是内容自适应的，即在推理过程中，值矩阵的权重与输入X相关。为了提供GFR模块的感应偏置，利用CA和FFN之间的深度卷积(DW)来补偿模块的空间性和局部性。使用快捷连接和元素加法，此外，还提出了一种解耦的群GFR (G-GFR)模块，其中GFR中FFN的FC层通过群卷积实现，群数为2。G-GFR可以增强TOD头部的全局表示能力，并通过群卷积将特征解耦成两个部分。\n\n\n\n\n损失函数\n\n该模型将计算三类损失，包括分类损失(Lcls)、对象损失(Lobj)和回归损失(Lreg)，总训练损失是它们的加权和。\n\n对象性损失:\n\n​                                       $$ L_{\\mathrm{obj}}&#x3D;-[y\\cdot\\log x+(1-y)\\cdot\\log(1-x)] $$\t\n​\t\t其中x为客观得分，y等于0或1，表示预测是缺陷或背景\n\n分类损失:\n​\t\t\t\t\t\t\t\t$$L_{\\mathrm{cls}}&#x3D;-\\sum_{i&#x3D;1}^c[y_i\\cdot\\log x_i+(1-y_i)\\cdot\\log(1-x_i)]$$\n​\tC表示缺陷类别的数量，x是预测的类置信度。y为软标签，其中目标类的标签为预测框与指定的地面真值框之间的IOU(交集)分数，其他类的标签为0。\n\n回归损失\n​\t\t\t\t\t\t\t\t$$L_{\\mathrm{reg}}&#x3D;1-\\mathrm{IOU}_{\\mathrm{pre}}^{gt}.$$\n​\t回归损失是预测边界盒(pre)与地面真值盒(gt)之间的IOU损失\n\n总损失函数\n​                                $$L_{\\mathrm{total}}&#x3D;\\frac{1}{N_{\\mathrm{pos}}}\\left(\\sum_{i&#x3D;1}^{N_{\\mathrm{pos}}}L_{\\mathrm{cls}}+\\lambda\\sum_{i&#x3D;1}^{N_{\\mathrm{pos}}}L_{\\mathrm{reg}}+\\sum_{i&#x3D;1}^{N}L_{\\mathrm{obj}}\\right)$$\n$N_{pos}$为SimOTA标签分配策略获得的阳性样本个数,N表示输入图像的总样本(预测)(特征映射的每个点将预测一个框),本文中的λ(根据经验设置为5)是回归损失的平衡权值\n实验\n数据集\nNEU-DET数据集共包含1800张图像，每张图像的大小为200 × 200像素\nSEDD包含7563张图像，分辨率为1920 × 1080。它收集了污水管道三种典型表面缺陷的图像:裂纹、根部和沉积物。\nAT-FDD数据集包括20种缺陷，如缺陷、磨损痕迹、结、翘曲、孔洞等。每张图像的大小为4096 × 1696，选取5913张图像进行评价\nNEU-DET用于评估ETDNet的检测能力(第IV-B节)和提议模块的有效性(第IV-C节)。SEDD和AT-FDD用于评估ETDNet的泛化能力(章节IV-E)。\n\n评价指标\n采用了平均精度(mAP)、模型大小(Params)、浮点运算(GFLOPs)和每秒帧数(FPS)作为评价指标。\n选择AdamW作为优化器，权重衰减为0.05。初始学习率为((1.6e−3)&#x2F;48)×批大小，批大小为24。我们使用余弦学习率计划，前20次热身，总训练时间为110次。为了避免过拟合，提高检测的鲁棒性，我们采用了四种数据增强方法，包括颜色抖动、随机水平翻转、MixUp和Mosaic。由于MixUp和Mosaic将破坏数据分布，在最后30个epoch关闭它们。最后的结果是五次实验的平均值，以4:1的比例进行评估\n\n\n","categories":["论文"],"tags":["torch","随笔","缺陷检测"]},{"title":"V8训练NEU-DET","url":"/2024/06/05/V8%E8%AE%AD%E7%BB%83NEU-DET/","content":"原版V8\n条件\nepcoh &#x3D;100,batch &#x3D;-1,model&#x3D;yolov8n\n\n结果\n\n\nmetrics/precision(B)\t      metrics/recall(B)\t       metrics/mAP50(B)\t    metrics/mAP50-95(B)   0.75987,                0.68248,                0.76654,                0.42516,  \n\n\n\n\n\nPR_recall\n\n\n\n\nConfusion Matrix Normalized\n\n\n条件2\nepoch &#x3D; 300，其余一样\n\n结果\n\n\nmetrics/precision(B)\t      metrics/recall(B)\t       metrics/mAP50(B)\t    metrics/mAP50-95(B)   0.73849,                0.65913,                0.72279,                0.39697,\n\n\n\nPR_recall\n\n\nConfusion Matrix Normalized\n\n\n\n\n可以看到当训练轮次增多的时候map反而降低了，验证损失波动比较大，合适的eopoch应该在100-150左右，\n\n\n加入GAM注意力\n条件\nepoch &#x3D; 100,batch &#x3D; 32,model&#x3D;yolov8n\n\n结果\nmetrics/precision(B),      metrics/recall(B),       metrics/mAP50(B),    metrics/mAP50-95(B),0.75814,                    0.69196,                  0.775,                0.41348,\n\n\n\nPR_recall\n\n\nConfusion Matrix Normalized\n\n\n加入SwinTransformer\n条件\nepoch &#x3D; 265,batch &#x3D; -1,model&#x3D;yolov8n\n\n结果\n\n\nmetrics/precision(B)\t      metrics/recall(B)\t       metrics/mAP50(B)\t    metrics/mAP50-95(B)         0.68036\t             0.72664\t             0.75465\t             0.39744\n\n\n\n\n\nPR_recall\n\n\nConfusion Matrix Normalized\n\n\n","categories":["CV"],"tags":["torch","缺陷检测"]}]